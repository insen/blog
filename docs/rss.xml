<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
    <title>Insen | [name, insen][url, https://linkedin.com/nilsengupta]</title>
    <link>https://insen.github.io/blog</link>
    <atom:link href="https://insen.github.io/blog/rss.xml" rel="self" type="application/rss+xml" />
    <description>Personal blog of [name, insen][url, https://linkedin.com/nilsengupta]</description>
    <language>en-us</language>
    <pubDate>Tue, 06 Nov 2018 18:45:57 +0530</pubDate>
    <lastBuildDate>Tue, 06 Nov 2018 18:45:57 +0530</lastBuildDate>
    
    <item>
      <title>Merge across repositories - Git.</title>
      <link>https://insen.github.io/blog/2018/10/31/Git-Merge-Across-Repos/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2018/10/31/Git-Merge-Across-Repos/</guid>
      <description>&lt;p&gt;** &lt;small&gt;&lt;em&gt;TL;DR - Skip straight to the last section.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;We all know that Git is an excellent piece of software and it is a DVCS and the D stands for &lt;em&gt;&apos;Distributed&apos;&lt;/em&gt; and all of that is very good and so on and so forth, etc.&lt;/p&gt;
&lt;p&gt;But honestly, how often do you use the &apos;D&apos; part in there?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The &apos;D&apos; in DVCS. A short detour!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Digressing a bit. Once upon a time when I was young, in programming or otherwise, I needed to handle distributed transactions. In the Microsoft world it meant DTC - the Distributed Transaction Coordinator and I didn&apos;t know squat-jack about that. So I walked up to the venerable senior architect in the organization and asked for  help. His terse response - &lt;em&gt;&lt;strong&gt;&apos;Change the system so that you don&apos;t need it !&apos;&lt;/strong&gt;&lt;/em&gt;. I was quite taken aback then, given that my naive faith in the power of tools had not yet been battle-tested.&lt;/p&gt;
&lt;p&gt;Over the years, this recurred once, then again, and again, and by now I have learned, both the hard way and the soft way, that the &lt;strong&gt;&lt;em&gt;&apos;D&apos;&lt;/em&gt;&lt;/strong&gt; basically means - &lt;strong&gt;&lt;em&gt;&apos;Don&apos;t go there. There be Dragons !!! &apos;&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So back to the point - &lt;em&gt;how do we use Git most often?&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;How do we use Git? Usually, mostly, as a better VCS.&lt;/h1&gt;
&lt;p&gt;In my experience, it works mostly as a drop in replacement for a standard VCS.&lt;/p&gt;
&lt;p&gt;Oh yes, it comes with value adds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Robust behavior&lt;/li&gt;
&lt;li&gt;Fast - try branching or checking out a big repo from Git vs SVN/VSTS.&lt;/li&gt;
&lt;li&gt;Cheap feature branches.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But mostly, in spite of all the &lt;em&gt;distribu-thing-ummajigs&lt;/em&gt; in there, we still have a central source of truth.&lt;/p&gt;
&lt;p&gt;Somewhere out there is a &apos;master&apos; which is &lt;em&gt;&apos;THE master&apos;&lt;/em&gt; branch.  And while we branch and fork and checkout and checkin and switch and &apos;pull request&apos; all  we like, sooner or later, all roads lead back to Rome - which in this case - is that particular &lt;em&gt;&apos;THE master&apos;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Short detour again - introducing Robert Frost !&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do you know this Frost?&lt;/p&gt;
&lt;p&gt;No? Ok, let me introduce you. The chief characteristic of the esteemed Mr. Frost is his rather well known penchant of taking the &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Road_Not_Taken&quot;&gt;road less tavelled by&lt;/a&gt;. (&lt;em&gt;Murphy must have been quite a fan !&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Point being, sometimes, Mr. Frost steps into the fray, and as is his wont, takes  the &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Road_Not_Taken&quot;&gt;road less travelled by&lt;/a&gt;, and pulls you along into it.&lt;/p&gt;
&lt;h1&gt;Road less travelled - Using Git as a &amp;quot;DVCS&amp;quot;.&lt;/h1&gt;
&lt;p&gt;So finally, after several years of using Git as a VCS with a first name (&amp;quot;D&amp;quot;), I finally have a situation where I have to deal with the &amp;quot;Distributed&amp;quot; nature of Git.&lt;/p&gt;
&lt;h2&gt;The context, and the problem.&lt;/h2&gt;
&lt;p&gt;Let me explain the situation. In this here and now, this is how the cookie crumbled.&lt;/p&gt;
&lt;p&gt;In my current engagement, work occurs in two Git repositories - &lt;em&gt;In two Git repositories&lt;/em&gt; - each with its own &apos;master&apos; branch and feature branches and history and so on.&lt;/p&gt;
&lt;p&gt;Why so? Well, because,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The work is &lt;strong&gt;&lt;em&gt;explorative&lt;/em&gt;&lt;/strong&gt; in nature.&lt;/li&gt;
&lt;li&gt;And there are multiple tracks of work for separate goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Many tracks of work &lt;strong&gt;DO NOT&lt;/strong&gt; have a deadline&lt;/em&gt;. It could be from &lt;em&gt;&apos;weeks&apos;&lt;/em&gt; to &lt;em&gt;&apos;years&apos;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Many of them &lt;strong&gt;MAY&lt;/strong&gt; never become production code&lt;/em&gt;. Production happens when certain conditions become evident.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And the explorative work has way more volume than actual production work. So to cleanly enforce separation of code, there are two repositories -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Repo Prod - The production code repository.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is all code that will be go to production at some point. All code is production quality, tests, CI/CD pipelines etc, but not all of them are in the &lt;em&gt;&apos;master&apos;&lt;/em&gt;, or &lt;em&gt;&apos;source of truth&apos;&lt;/em&gt; branch.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repo ConDev - The concept development repository.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Again lots of branches, with master, and feature branches, and spikes, and prospective candidates for production, and abandoned work and so on. As the name indicates, it is for concept development.&lt;/li&gt;
&lt;li&gt;rules/coding guidelines/practises are a lot more lax here. Essentially, this if for feasibility studies, investigative work and other POCs (Proof-Of-Concept).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But every now and then, a POC clears business validation.&lt;/p&gt;
&lt;p&gt;Subsequently, a new branch in created in the POC repo, within which the concept based code is upgraded and polished, and code quality reviews and all of the other practises which production readiness demands happen.&lt;/p&gt;
&lt;p&gt;Once that is done, though, what we have is this nice, freshly minted, production quality  sub-system residing in branch &apos;nice-production-quality-sub-system&apos; in repository &apos;POC&apos;.&lt;/p&gt;
&lt;p&gt;What we need, however, is the above code sitting in branch &apos;checked-in-production-sub-system&apos; in repository &apos;Prod&apos; - along with its long history of development in the &apos;POC&apos; repo.&lt;/p&gt;
&lt;p&gt;Thus what I have to do is &lt;strong&gt;move the code across git repositories, preserving history.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Eh! WHAT?&lt;/p&gt;
&lt;h2&gt;The theory - Git merge across repositories.&lt;/h2&gt;
&lt;p&gt;What is the first thing to do when you have something like this? Well, you look for volunteers. There are none. Ok, So someone must have done this before? Oops ! Double or nothing, its nobody again. Thus at this point, after successfully drawing two blanks, you go back to the drawing board and start doodling.&lt;/p&gt;
&lt;p&gt;Which is when it hits you. Git is designed to do exactly that - it is, after all, a distributed VCS.&lt;/p&gt;
&lt;p&gt;All version control systems have this concept of &apos;local repository&apos; and &apos;remote repository&apos;. The way you work, is that you make all changes in your local copy, and when it is done you check in into local. This local is something on your local physical machine. and at this point - only your copy has your changes.&lt;/p&gt;
&lt;p&gt;But usually you want to synchronize things, Not just make endless changes to the same thing (repository). And synchronize implicitly means you needs two things to synchronize between - a &apos;source&apos; and a &apos;sink&apos;.&lt;/p&gt;
&lt;p&gt;From wikipedia -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Distributed revision control systems (DVCS) takes a peer-to-peer approach to version control, as opposed to the clientâ€“server approach of centralized systems. Distributed revision control synchronizes repositories by exchanging patches from peer to peer. There is no single central version of the codebase; instead, each user has a working copy and the full change history.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;And here is the trick that actually lets us do what we want.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In traditional or client-server based synchronization, you can only synchronize from the &apos;sink&apos; to the &apos;source&apos;. The &apos;sink&apos; can keep changing its contents, and you can have many &apos;sink&apos;s, but every single &apos;sink&apos; needs to pass all its changes/records to the &apos;source&apos; for synchronization to be effective and available to all other &apos;sink&apos;s. And these synchronization steps are all sequential. The &apos;source&apos; is the gatekeeper, and cannot be changed.&lt;/p&gt;
&lt;p&gt;Git however, eliminates, the difference between &apos;source&apos; and &apos;sink&apos; - &apos;source&apos; and &apos;sink&apos; are just roles you play. Moreover, any repository can choose any other repository, and between those two, either party may play the role of &apos;source&apos; or &apos;sink&apos; at will.&lt;/p&gt;
&lt;p&gt;Some taxonomy now - In Git - the &apos;sink&apos; is called the &apos;local&apos; repo. The &apos;source&apos; is called the &apos;remote&apos;. And in Git or other DVCSs it is possible to switch out the &apos;remote&apos; to some other &apos;remote&apos;, or have any number of &apos;remote&apos;s.&lt;/p&gt;
&lt;p&gt;On that principle, if I fetch the &lt;em&gt;&apos;prod-ready-poc&apos;&lt;/em&gt; branch into the &lt;em&gt;&apos;POC&apos; local repo&lt;/em&gt; from the &lt;em&gt;&apos;POC&apos; remote&lt;/em&gt;, then switch (or actually, add) the &lt;em&gt;&apos;Prod&apos; remote&lt;/em&gt; as another &apos;source&apos; - call it &apos;newremote&apos; - it should theoretically be possible to push the now local &lt;em&gt;&apos;prod-ready-poc&apos;&lt;/em&gt; branch from &lt;em&gt;&apos;POC&apos; local&lt;/em&gt; into a new branch - call it &lt;em&gt;&apos;new-in-prod&apos;&lt;/em&gt; - on the &lt;em&gt;&apos;Prod&apos; remote&lt;/em&gt; i.e. the &lt;em&gt;&apos;newremote&apos;&lt;/em&gt;. Git should, in principle, seamlessly merge both.&lt;/p&gt;
&lt;p&gt;Or that&apos;s the theory anyway.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;small&gt;A complete discussion of why is beyond the scope of this post - but if you are interested you can get started with &lt;a href=&quot;https://git-scm.com/book/en/v2/Getting-Started-Git-Basics&quot;&gt;git basics&lt;/a&gt;. Another good source is &lt;a href=&quot;https://www.atlassian.com/git/tutorials/setting-up-a-repository/git-clone&quot;&gt;here&lt;/a&gt;&lt;/small&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Walking the talk - Git merge across repositories.&lt;/h2&gt;
&lt;p&gt;But theory and practise are very different things, so let&apos;s see what it takes to put theory into practise.&lt;/p&gt;
&lt;h3&gt;Arrange&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Create two repositories in some remote location - &apos;prod&apos; and &apos;cd&apos;. Each with just the master branch.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For my purposes, everything is setup on local machine - all 4 instances of the git repos are on my local file system. After setup, it looks like this :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/posts/repos-file-structure.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work with &apos;prod&apos;
&lt;ul&gt;
&lt;li&gt;Get &apos;prod&apos; onto local machine - &apos;locprod&apos;.&lt;/li&gt;
&lt;li&gt;Add folder &apos;f1&apos; and &apos;f2&apos; to &apos;master&apos; in &apos;locprod&apos;.&lt;/li&gt;
&lt;li&gt;Add files &apos;f11&apos; and &apos;f21&apos; to &apos;master&apos; in &apos;locprod&apos;.&lt;/li&gt;
&lt;li&gt;Create branch &apos;pb&apos; in &apos;locprod&apos; from &apos;master&apos;.&lt;/li&gt;
&lt;li&gt;push &apos;pb&apos; from &apos;locprod&apos; to &apos;prod&apos;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After setup, the history for branch &apos;pb&apos; from &apos;locprod&apos; is :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/posts/locprod-after-setup.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now work with &apos;cd&apos;
&lt;ul&gt;
&lt;li&gt;Get &apos;cd&apos; onto local machine - &apos;loccd&apos;.&lt;/li&gt;
&lt;li&gt;Add folder &apos;f3&apos; and &apos;f4&apos; to &apos;master&apos; in &apos;lcocd&apos;.&lt;/li&gt;
&lt;li&gt;Add files &apos;f31&apos; and &apos;f41&apos; to &apos;master&apos; in &apos;loccd&apos;.&lt;/li&gt;
&lt;li&gt;Create branch &apos;cdb&apos; in &apos;loccd&apos; from &apos;master&apos;.&lt;/li&gt;
&lt;li&gt;push &apos;cdb&apos; from &apos;loccd&apos; to &apos;cd&apos;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And branch &apos;cdb&apos; history from &apos;loccd&apos; is so :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/posts/loccd-after-setup.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;At this point we have two repositories with branches whose histories have diverged. The goal is to get all the contents and history of &apos;cdb&apos; in &apos;loccd&apos; into &apos;pb&apos; in &apos;prod&apos;.
Which means, at the end, the &apos;pb&apos; branch in &apos;prod&apos; should contain all the four folders &apos;f1&apos;, &apos;f2&apos;, &apos;f3&apos;, &apos;f4&apos; with all of their files, and the history of branch &apos;pb&apos; should contain all of the individual commit/operation logs from &apos;prod&apos;/&apos;locprod&apos; and &apos;cd&apos;/&apos;loccd&apos;.&lt;/p&gt;
&lt;p&gt;I am not showing the steps for the above because if you need help there, you are really not ready for the next bits.&lt;/p&gt;
&lt;h3&gt;Act&lt;/h3&gt;
&lt;p&gt;The goal here is to merge &apos;loccd&apos;.&apos;cdb&apos; into, ultimately, &apos;prod&apos;.&apos;pb&apos;. However, this is a multi-step process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;While on bash in &apos;locprod&apos;.&apos;master&apos;, add a new EMPTY branch &apos;tmp&apos; in &apos;locprod&apos; &lt;code&gt;git checkout --orphan tmp&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Switch to bash on &apos;loccd&apos;. Add &apos;locprod&apos; as a new remote repo for &apos;loccd&apos;. Name this remote as &apos;nremote&apos;. &lt;code&gt;git remote add nremote /full/file/path/to/gitrepo/.git&lt;/code&gt; To verify remotes are setup, run &lt;code&gt;git remote -v&lt;/code&gt;. On my machine, I see the
&lt;img src=&quot;/blog/img/posts/all-remotes.png&quot; alt=&quot;following&quot; /&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fetch all from the newly configured remote. &lt;code&gt;git fetch nremote&lt;/code&gt;
Viewing list of branches now shows this.
&lt;img src=&quot;/blog/img/posts/all-branches-with-nremote.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create new branch &apos;loccd&apos;.&apos;tmp&apos; from &apos;loccd&apos;.&apos;cdb&apos; &lt;code&gt;git checkout -b tmp&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set it to track the branch &apos;tmp&apos; on &apos;locprod&apos;. &lt;code&gt;git push -u nremote tmp&lt;/code&gt;. Ensure that the target branch is not open  -i.e. &apos;tmp&apos; is not the currently checked out branch for &apos;locprod&apos;. If so, this is the error you get. &lt;img src=&quot;/blog/img/posts/error-open-branch.png&quot; alt=&quot;&quot; /&gt; Switch to some other branch. Once this step is successfull you have the &apos;loccd&apos;.&apos;tmp&apos; data in &apos;locprod&apos;.&apos;tmp&apos;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to bash on &apos;loccd&apos;.&apos;tmp&apos; and merge &apos;loccd&apos;.&apos;cdb&apos; into &apos;loccd&apos;.&apos;tmp&apos;. Assuming you are on the &apos;tmp&apos; branch in bash, run &lt;code&gt;git push nremote tmp --allow-unrelated-histories&lt;/code&gt; . Note the flag. Once past this step, &apos;locprod&apos;.&apos;tmp&apos; should have everything from &apos;prod&apos;.&apos;pb&apos; and &apos;locprod&apos;.&apos;cdb&apos;, including histories.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Push everything back into &apos;prod&apos;.&apos;tmp&apos;. Since &apos;tmp&apos; was created in  &apos;locprod&apos;, it doesn&apos;t exist yet in &apos;prod&apos;, so you need to set the upstream tracking info. &lt;code&gt;git push -u origin tmp&lt;/code&gt; creates the branch if it doesnt exist and starts tracking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Switch bash to &apos;locprod&apos;.&apos;pb&apos;. Reverse merge &apos;locprod&apos;.&apos;tmp&apos; into &apos;locprod&apos;.&apos;pb&apos;.
&lt;code&gt;git merge tmp&lt;/code&gt;. This should be seamless.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Assert&lt;/h3&gt;
&lt;p&gt;And you are done.&lt;/p&gt;
&lt;p&gt;This is how the final folders look.
&lt;img src=&quot;/blog/img/posts/post-merge-folders.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;And this is the final log of history on &apos;prod.pb&apos;.
&lt;img src=&quot;/blog/img/posts/post-merge-log.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;And this is the final git merge tree.
&lt;img src=&quot;/blog/img/posts/post-merge-tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some Notes
&lt;ul&gt;
&lt;li&gt;Everything from both repos is merged and available in &apos;prod.pb&apos;.&lt;/li&gt;
&lt;li&gt;There may be a few redundant steps in between, but I was gunning for a solution where, in case of issues, I could just delete stuff and restart - hence all thise temporary branches. It could conceivably work without those, but your mileage may vary.&lt;/li&gt;
&lt;li&gt;One precaution - Always work off clean branches with no pending changes. Stage or commit everything before starting on this.- You are just one step away from having everything in &apos;prod&apos;.&apos;master&apos; by merging &apos;prod.pb&apos; into &apos;prod&apos;.&apos;master&apos;. Which you can do by whatever method works best for you - review, merge, push, pull-request whatever.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is what worked for me. Thank you for reading. Have a nice day!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Small but irritating things - Git.</title>
      <link>https://insen.github.io/blog/2018/07/18/Small-But-Irritating-Things/</link>
      <pubDate>Wed, 18 Jul 2018 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2018/07/18/Small-But-Irritating-Things/</guid>
      <description>&lt;p&gt;I tried to retrieve a stash by name in Git. Honest, thats all I did.&lt;/p&gt;
&lt;p&gt;Yet here I am.&lt;/p&gt;
&lt;p&gt;Software is full of small and intensely irritating things.  Repeat after me - &lt;em&gt;Software is full of small and intensely irritating things.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You will encounter it again and again, in good software, bad software, throw away software and never used software &lt;em&gt;(for obvious reasons)&lt;/em&gt;. And you will encounter it in the best of software too.&lt;/p&gt;
&lt;p&gt;Which, from now onwards, I will be documenting.&lt;/p&gt;
&lt;p&gt;And today is a Git day.&lt;/p&gt;
&lt;h2&gt;Short historical background on GIT.&lt;/h2&gt;
&lt;p&gt;Lets digress -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The person who wrote Git was, or rather, is the most famous programmer alive and quite possibly, the most respected as well. Name&apos;s Linus Torvalds.&lt;/li&gt;
&lt;li&gt;Git again is like a cross between the Mona Lisa and the Swiss Army Knife (&lt;em&gt;the sort which Schwarzenneger carries into a Predator jungle and builds a seven storeyed Harrods with&lt;/em&gt;). Its an engineering marvel; Considering the distributed part, its a conceptual marvel; its incredibly robust; It&apos;s blazing fast and its very versatile.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So given these ancestors and these antecedents, you would assume that Git is flawless - right? And its venerable primary author and the plethora of talented programmers who have lovingly grafted their art onto the foundations laid by the erstwhile venerable one, have paid complete attention to both functionality and ease of use which is why you have a billion flags (options) with each command, and there are at least 7 ways to do any one thing (exaggerating only a bit), and its rock solid - correct? Well, yes, its all of that, but inspite of all that &lt;em&gt;&lt;strong&gt;try, just try, to retrieve a stash by name&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Small but intensely irritating.&lt;/h2&gt;
&lt;p&gt;Here is the small but intensely irritating part - &lt;em&gt;&lt;strong&gt;there is no easy way to do it.&lt;/strong&gt;&lt;/em&gt;. The best you can do is find the first one that matches a RegEx.&lt;/p&gt;
&lt;p&gt;That&apos;s odd (&lt;em&gt;and I am being polite here&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;In Git, arguably one of the best softwares on the planet, &lt;em&gt;you can easily add a key-value pair&lt;/em&gt;, but &lt;em&gt;you cannot easily retrieve it by key&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;What?&lt;/p&gt;
&lt;p&gt;Let me rephrase this - You are using an excellent programming language. Probably the best ever. In that language there is a dictionary or hashtable or map implementation. It allowes you to stuff things into a key-value collection by a key (random string). But here&apos;s the catch, it does not give you way to retrieve the same by name. Not easily anyway.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now list down your opinion of the programmer, designer and architect of the person who did this. And then...&lt;/li&gt;
&lt;li&gt;Map the attributes you wrote down against the names Linus Torvalds or Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(The second bullet point above would probably result in something resembling that OCD nightmare meme - a leaning tower of Pisa in a square box in a square wall picture frame. IT. DOES. NOT. COMPUTE. How did these people miss this?)&lt;/p&gt;
&lt;h2&gt;Software has issues. Period.&lt;/h2&gt;
&lt;p&gt;Ok, hyperbole apart, this is basically what software is.&lt;/p&gt;
&lt;p&gt;Take the best software, built by the best people - It doesn&apos;t matter. Something, somewhere is inevitably going to fall off the radar, or the planning, or the execution, or the design.&lt;/p&gt;
&lt;p&gt;In this case Git is probably the best DVCS out there, conceived by a marquee brand leader, built and maintained over many years by a great team, yet the fact that if you stored something by name, you would want to retrieve it by name seems to have escaped all those people who participated in it over all the years.&lt;/p&gt;
&lt;p&gt;And, as is usually the case, it never became important enough to get put back in.&lt;/p&gt;
&lt;h2&gt;So, Git Unstash.&lt;/h2&gt;
&lt;p&gt;Yeah, ok software got issues. But we got developers. And developers fix issues. &lt;small&gt;&lt;em&gt;And what&apos;s a technical blog post without a solution to an issue anyway?&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;So here&apos;s how to do unstash in Git.&lt;small&gt; &lt;em&gt;(even though its available just a couple of google clicks away).&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
&lt;h3&gt;Git - stash by name.&lt;/h3&gt;
&lt;p&gt;This works just so -&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash save &amp;quot;thisName&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The above is deprecated but still works. The current recommended way is -&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash push &amp;quot;thisName&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Git - unstash by name.&lt;/h3&gt;
&lt;p&gt;This is the iffy thing. you can do this -&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash apply stash^{/thisNa}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Notice the &lt;em&gt;&apos;thisNa&apos;&lt;/em&gt;, that&apos;s a regex which contains part of your stash message. Additionally, what this does is retrieve the first named stash value which matches. Which means, if you do this&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash save &apos;thisName1&apos;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and this&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash save &apos;thisName2&apos;&lt;/code&gt;,&lt;/p&gt;
&lt;p&gt;You are out of luck during retrieval unless you can remember the exact second phrase, like this -&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash apply stash^{/thisName2}&amp;quot;&lt;/code&gt;,&lt;/p&gt;
&lt;p&gt;Since Git returns only the first match, using part of the phrase leads you to the first partial match which may or may not be what you want.&lt;/p&gt;
&lt;p&gt;There are alternatives which involve using a combination of&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash list&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git stash pop/apply stash@{n}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where n is a number referring to nth stash from top, etc (&lt;em&gt;and use either pop or apply, not both&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;For additional tips and sources and other more detailed knowhow, look up this &lt;a href=&quot;https://stackoverflow.com/questions/11269256/how-to-name-and-retrieve-a-stash-by-name-in-git&quot;&gt;SO question&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Bottomline.&lt;/h2&gt;
&lt;p&gt;Software, even the best software, has nuances and idiosyncrasies. Stop cribbing, drink your medicine, and get back to work(arounds).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Windows Impersonation and The Selfish Test</title>
      <link>https://insen.github.io/blog/2018/05/16/The-Selfish-Test/</link>
      <pubDate>Wed, 16 May 2018 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2018/05/16/The-Selfish-Test/</guid>
      <description>&lt;p&gt;Well, you know, there&apos;s automated testing. And then there&apos;s &lt;em&gt;&apos;this&apos;&lt;/em&gt; automated testing. And then there is &lt;em&gt;&apos;that&apos;&lt;/em&gt; automated testing. And then there is &lt;em&gt;&apos;this-and-that&apos;&lt;/em&gt; automated testing. And then there are the debates about &lt;em&gt;&apos;this&apos;&lt;/em&gt; vs &lt;em&gt;&apos;that&apos;&lt;/em&gt; vs &lt;em&gt;&apos;this-and-that&apos;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Except for the fact that, nobody really tells you what&apos;s in it for you.&lt;/p&gt;
&lt;p&gt;After all it is just a boatload more work. And frankly, nobody pays for work which exists just to prove &lt;a href=&quot;https://en.wikipedia.org/wiki/Parkinson%27s_law&quot;&gt;Parkinson&apos;s Law&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Which brings us to the million dollar question - What&apos;s in it for the dear programmer, toiling away to finish a ticket?&lt;/p&gt;
&lt;p&gt;Welcome to the programmers day job.&lt;/p&gt;
&lt;h2&gt;The Programmers day job - 101.&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Along came a context...&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You are relatively new into a project.&lt;/p&gt;
&lt;p&gt;A ticket arrives in a sprint. Blessed by the hands of the product owner. The basic requirement is to be able to run a file copy operation in windows in C# under an account different from the logged in users.&lt;/p&gt;
&lt;p&gt;So we immediately raise our hands - Windows Impersonation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Eh? right ! Slow down a bit, will you. Lets finish with the context, ok?.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This file is the output file of a long operation, - a long, multiple step, time delayed process.&lt;/li&gt;
&lt;li&gt;Which aggregates data from multiple APIs.&lt;/li&gt;
&lt;li&gt;And does a bunch of batch processing and tansformations on that.&lt;/li&gt;
&lt;li&gt;And then displays and accepts user intervention.&lt;/li&gt;
&lt;li&gt;And then redoes a bunch of calculations based on the user intervention, if any.&lt;/li&gt;
&lt;li&gt;And at the end of it all creates a file in memory.&lt;/li&gt;
&lt;li&gt;And then reads some configuration file settings whose keys are determined by part of the in-memory file newly created.&lt;/li&gt;
&lt;li&gt;And then based on that configuration data determines a remote target network folder.&lt;/li&gt;
&lt;li&gt;And then based on this remote folders location, switches to the corresponding configured account, which has folder write access, to copy this generated file to the target remote folder.&lt;/li&gt;
&lt;li&gt;Oh! And the method which copies the file over is buried deep in code as a private method which is called at the end of this entire process.&lt;/li&gt;
&lt;li&gt;And no, not every line of this code-base has automated tests. &lt;em&gt;You come up with your list of whys and therefores, and chances are we&apos;ll have a few in common&lt;/em&gt;. The testing on this is a mix of unit, API, integration, selenium and manual testing.&lt;/li&gt;
&lt;li&gt;And the existing code-base which runs this uses windows integration which needs a complex Active Directory setup which does not exist. It has a way of falling back to a hard-coded account based authentication implementation for local testing purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your job is to run that private method under a different user-account. &lt;em&gt;Lets hear it again now for that Windows Impersonation thing.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Welcome to a programmers life, my friend, &lt;em&gt;Its all about the context !&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Solutioning&lt;/h2&gt;
&lt;p&gt;You need a change in code, but, as a developer, you cannot just add the code and hand it over to QA - you need to be sure that your code works. And thus, you have a few problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can you change private method accesibility? Only locally, you may not check-in.&lt;/li&gt;
&lt;li&gt;Can you run the full system? No you cannot. You don&apos;t have the AD, the local authentication scheme is too limited to be sufficient to test. The actual process in production runs within a windows service which is configured to use a specific system windows account with elevated and customized permissions.&lt;/li&gt;
&lt;li&gt;Can you run the entire operation from end to end? No, you cannot, the data setup costs will be prohibitively expensive in terms of man-hours.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Development&lt;/h3&gt;
&lt;p&gt;Going by the &lt;em&gt;&lt;strong&gt;&apos;Principle of least interference&apos;&lt;/strong&gt;&lt;/em&gt;. the code change should preferably be limited to the code in that &lt;code&gt;private&lt;/code&gt; method, encapsulating the copy operation. &lt;em&gt;And let&apos;s just go ahead and do the hard thing first and name the method &lt;code&gt;FileCopy&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now you start by writing a wrapper library around the basic Windows Impersonation logic from the &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/w070t6ka(v=vs.110).aspx&quot;&gt;Microsoft Windows Impersonation documentation&lt;/a&gt;. Better still - don&apos;t write it. Someone&apos;s already written one &lt;a href=&quot;https://github.com/mj1856/SimpleImpersonation&quot;&gt;here&lt;/a&gt;. A simple API based on the  &lt;code&gt;using&lt;/code&gt; statement gives you nice, elegant syntax, and the ability to wrap any block of code in an &lt;em&gt;impersonation&lt;/em&gt; section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;using (Impersonation.LogonUser(domain, username, password, logonType))
{
  // do whatever you want as this user.```
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;(Note that there is debate about using &lt;code&gt;using&lt;/code&gt; this way - find discussed &lt;a href=&quot;https://stackoverflow.com/questions/2101524/is-it-abusive-to-use-idisposable-and-using-as-a-means-for-getting-scoped-beha&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://grabbagoft.blogspot.in/2007/06/example-of-creating-scope-with-using.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://stackoverflow.com/questions/1095438/bad-practice-non-canon-usage-of-cs-using-statement&quot;&gt;here&lt;/a&gt;.)&lt;/em&gt;,&lt;/p&gt;
&lt;p&gt;So off you go -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change the &lt;code&gt;private&lt;/code&gt; method to &lt;code&gt;internal/public&lt;/code&gt; in your class. &lt;em&gt;You will change it back before check-in&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Wrap the file copy operation in an impersonation block as per the usage shown above.&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;InternalsVisibleTo&lt;/code&gt; attribute to the assembly, if you change the method to &lt;code&gt;internal&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Configure the &lt;code&gt;app.settings&lt;/code&gt; keys which provide the &lt;em&gt;user name&lt;/em&gt; and &lt;em&gt;password&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Infrastructure&lt;/h3&gt;
&lt;p&gt;You have asked your network admin to give you a test-account on the network, and you have configured all settings, and you are all set.&lt;/p&gt;
&lt;h3&gt;Verification&lt;/h3&gt;
&lt;p&gt;Now comes the difficult part - &lt;em&gt;How to verify a file-copy was by a specific account ?&lt;/em&gt; - when you cannot execute the whole system or the whole code.&lt;/p&gt;
&lt;p&gt;You start writing an automated test, because, sometimes, there is just no other viable way to verify your code. Interestingly, however, in this particular context, it is also the most complicated coding involved in this particular ticket. Yet it is effectively unavoidable as all other ways of verifying the code you wrote are not feasible on account of time constraints.&lt;/p&gt;
&lt;p&gt;So lets structure the test.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Arrange&lt;/strong&gt; - For a start, we need to instantiate the target class. &lt;em&gt;Luckily there was master suite of tests which needed this class so class instantiation with all dependencies was a solved problem.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Act&lt;/strong&gt; - Execute the &lt;code&gt;FileCopy&lt;/code&gt; operation. The &lt;code&gt;FileCopy&lt;/code&gt; operation reads in the user account information, creates a new impersonation block and executes the file copy operation with that scope.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assert&lt;/strong&gt; - Read in the  windows file system attributes and compare the file Created By attribute with the configured account for the file copy. They should match and we should be done.&lt;/p&gt;
&lt;p&gt;In real life, you still have a few problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, figure out the exact lines of code involved and&lt;/li&gt;
&lt;li&gt;Some peculiarities with how windows user accounts work, especially ones setup as administrators.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But at this point lets get down to sample code. Find below the relevant snippets of the program and the test code, well commmented, and hopefully, self explanatory.&lt;/p&gt;
&lt;h3&gt;The Program Code&lt;/h3&gt;
&lt;p&gt;First, the program code - the part that goes into production, and the simpler part of the coding bit -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Public class BigBadClassWithLotsOfInitialization
{
    ...
    ... OTHER CODE ELIDED ...
    ...

    //Note: this is a private method. We change it to public/internal 
    //to test. Once done, we switch it back to private before check-in.
    public void FileCopy(string localXmlFilePath, string newPath)
    {
        string domainAndUser = Cfg.GetServiceUserName();
        string domain = domainAndUser.Split(&apos;\\&apos;)[0];
        string user = domainAndUser.Split(&apos;\\&apos;)[1];
        string password = Cfg.GetServiceUserPwd();
        LogonType logonType = LogonType.Interactive;

        //Impersonation block.
        using (Impersonation.LogonUser(domain, user, password, logonType))
        {
            // pr-existing code.

            var fileName = Path.GetFileName(localXmlFilePath);
            if (fileName != null)
                newPath = Path.Combine( newPath
                               ,fileName.Replace(&amp;quot;.xml&amp;quot;, GetTimestamp() + &amp;quot;.xml&amp;quot;));

            Logger.InfoFormat(&amp;quot;Moving file {0} to {1}&amp;quot;, localXmlFilePath, newPath);

            if (File.Exists(newPath))
            {
                File.Delete(newPath);
            }
            File.Move(localXmlFilePath, newPath);

            // end pr-existing code.
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;The Test Code&lt;/h3&gt;
&lt;p&gt;This is the part of the code that goes no-where, and doesn&apos;t even work after its checked-in. And ironically, the more complex part of the coding, mainly because of the Windows file system know-how involved.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[Test, Ignore]
public void CanWriteFileAsDifferentUserThroughJob()
{
    //Notes | This is a shortcut to test whether elevated permissions 
    //are happening when &apos;FileCopy&apos; is called. &apos;FileCopy&apos; is a private 
    //method. So we need to change to public when we need to test. 
    //Change it back when done.
    var job = new BigBadClassWithLotsOfInitialization();

    //Needs to be xml file.
    var fname = Guid.NewGuid().ToString();
    var fileName = fname + &amp;quot;.xml&amp;quot;; 

    var fromPath = Cfg.GetAutomaticPlaceOrdersExportOutputDirectoryPath();
    var fromFile = Path.Combine(fromPath, fileName);

    //using File.Create opens a fileStream and keeps it so, thus 
    //causing file cannot be used errors. this appends if it 
    //exists. It is ok to keep creating without deleting as 
    //MoveFile deletes source file after successful move.
    using (StreamWriter sw = new StreamWriter(fromFile, true))
    {
        sw.Write(fname);
    }

    var toPath = Cfg.GetAutomaticPlaceOrdersRemoteDirectoryPath(440);

    //NOTE 1: this line does not compile when the MoveFile method is 
    //private. so you need to change it before running the test.
    //NOTE 2: when using an account which does not have write access 
    //to the target folder, this line throws an exception. so if the 
    //user account set in config does not have permission, this line 
    //throws an error. If it has permission then write happens ok, but 
    //you encounter the subsequent notes.
    job.FileCopy(fromFile, toPath);

    var toFile = new DirectoryInfo(toPath
                     .GetFiles()
                     .First(e =&amp;gt; e.Name.Contains(fname));
    var fs = File.GetAccessControl(toFile.FullName);
    var idRef = fs.GetOwner(typeof(SecurityIdentifier))
                  .Translate(typeof(NTAccount));
    
    //this part is not working. why? Because the only other domain 
    //account i have is also an administrator. when doing stuff with 
    //administrator accounts, windows upgrades the user detail to the 
    //administator group. For details check stackoverflow - https://stackoverflow.com/questions/3370146/how-can-i-find-out-who-created-a-file-in-windows-using-net for more details. So following assert doesnt work.
    //Assert.AreEqual(Cfg.GetServiceUserName(), idRef.Value);

    //This does.
    Assert.AreEqual(&apos;BUILTIN\Administrators&apos;, &apos;idRef.Value&apos;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thats all of the significant code. It might take some work to put into a consistently reproducible state - a class library, a test project. some settings, a couple of user accounts on your machine. But most of that should be relatively standard.&lt;/p&gt;
&lt;h2&gt;The Selfish Test&lt;/h2&gt;
&lt;h2&gt;&lt;em&gt;a.k.a Automated Testing, By Developer, For Developer&lt;/em&gt;.&lt;/h2&gt;
&lt;p&gt;So coming back to the question - what&apos;s in it for you, the developer?&lt;/p&gt;
&lt;p&gt;so lets consider the test above -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Is it an automated test? &lt;strong&gt;Yes&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Is it an unit test? &lt;strong&gt;No&lt;/strong&gt;.  You will be changing the &lt;code&gt;FileCopy&lt;/code&gt; method to private and the test to &lt;code&gt;Ignore&lt;/code&gt; before check-in. You will also be commenting out the call to the &lt;code&gt;FileCopy&lt;/code&gt; method withing the test, as otherwise all you will be distributing to your team members is a compile time error. At point, its hardly even a test. Just some dead-code in the system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Is it an integration test? &lt;strong&gt;Possibly&lt;/strong&gt;. Why? It does integrate with the file system and active directory accounts, but the point is moot since the fact that you change the method to private and the test to ignore and comment out the &apos;&lt;em&gt;act&lt;/em&gt;&apos; stage of the test &lt;em&gt;renders it unusable for anything after check-in&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Is it easy to write? &lt;strong&gt;No&lt;/strong&gt;. In this case, it is the most complicated part of the code written for that ticket.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BUT CAN WE AVOID IT? &lt;strong&gt;NO&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Simply, because none of the other approaches to verification are feasible options.&lt;/p&gt;
&lt;p&gt;So exactly what sort of automated test is this? What benefits does it provide? Whom does it help?&lt;/p&gt;
&lt;p&gt;The answer is rather selfish. This test, gives you, the developer -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The ability to execute the code you added in &lt;code&gt;FileCopy&lt;/code&gt;without lots of expensive setup.&lt;/li&gt;
&lt;li&gt;The ability to step-into and debug the code line by line if necessary.&lt;/li&gt;
&lt;li&gt;And since the change has a small surface area, the code tested by this can be reasonably expected to reproduce similiar behavior in pre-production and production environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And thus we have &lt;strong&gt;The Selfish Test&lt;/strong&gt; - &lt;strong&gt;&lt;em&gt;Automated verification, by developer, for developer&lt;/em&gt;&lt;/strong&gt;. The above is an example of a &lt;em&gt;Selfish Test&lt;/em&gt;. You write this to help yourself. This is the only cheap, easy way to help you to debug as well as to verify the code you wrote. Its lifetime and use is limited to the duration of your development time.  Its only purpose is to help you.&lt;/p&gt;
&lt;p&gt;So, dear developer, go help yourself.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hello SARG ! Let&apos;s index and tune a SQL!</title>
      <link>https://insen.github.io/blog/2018/04/12/Hello-SARG/</link>
      <pubDate>Thu, 12 Apr 2018 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2018/04/12/Hello-SARG/</guid>
      <description>&lt;p&gt;No, Not your Beetle Bailey Sarge ! Focus !! Here we have composite keys and query plans and cryptic acronyms all playing ball really well together, only in their own fashion, much like today&apos;s unsupervised machine learning methods which nobody really understands.&lt;/p&gt;
&lt;h4&gt;The pole position &lt;em&gt;(&apos;Pole Problem&apos;??)&lt;/em&gt;.&lt;/h4&gt;
&lt;p&gt;The story begins with LINQ and composite keys in EF. (&lt;small&gt;in fact, many programmer stories usually begin with LINQ. Itâ€™s a veritable treasure trove of stories&lt;/small&gt;). So one day I needed the LINQ equivalent of a SQL IN query for a composite keyed table. That&apos;s what started this whole SARG thing.&lt;/p&gt;
&lt;p&gt;Consider a table with a single column primary key called Id. The query to get records with a IN clause via LINQ is this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    from entity in db.Table 
    where computedList.Contains(entity.Id) 
    select x.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, extrapolating merrily, you reason that tables with Composite keys would need a join as you can&apos;t match multiple values in the contains option. And so you write&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    from entity in db.Table
    join pair in locals 
    on new { entity.Id1, entity.Id2 }
    equals new { Id1 = pair.Item1, Id2 = pair.Item2 }
    select entity
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Boom! Error message 101. (&lt;small&gt; ps. Did you ever hear that joke about the guy who wanted to write great things, move people to joy and tears through his words and ended up writing error messages for Microsoft&lt;/small&gt;). Here is an example in action - the result of executing the above LINQ is an error ending with &lt;code&gt;	...Only primitive types or enumeration types are supported in this context.&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What it means here is EF and SQL Server have no idea how to join a table in database with a list in memory. It cannot be done. You can either have a SQL query (generated through the IQueryable implementation of LINQ) or you fetch all database data into memory and then do the join. The latter is a very bad idea in a table with even 200000 rows, let alone millions.&lt;/p&gt;
&lt;p&gt;So how do you write the LINQ such that you get SQL which runs in SQL server? Turns out that this problem is quite intractable, with no elegant solutions.&lt;/p&gt;
&lt;p&gt;You have one SQL equivalent which uses LINQ&apos;s &apos;Contains&apos; option awkwardly&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    from entity in db.Table
    where computed.Contains(&amp;quot;Id1=&amp;quot; + entity.Id1 + &amp;quot;,&amp;quot; + &amp;quot;Id2=&amp;quot; + entity.Id2)
    select entity
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, you have created your list &apos;computed&apos; with key values which are a concatenation of all key columns as strings. In your LINQ query, you create equivalent keys from all of the database rows and compare this generated key with your predefined list. This generates a SQL of the sort&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    Select * from Table 
    Where (&amp;quot;Id1=&amp;quot; + Id1 + &amp;quot;,&amp;quot; + &amp;quot;Id2=&amp;quot; + Id2) 
    in ( &amp;quot;Id1=Id1,Id2=Id2&amp;quot;, &amp;quot;Id1=Id91,Id2=Id92&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;small&gt;&lt;em&gt;Note:&lt;/em&gt; Not exact, but thatâ€™s handwritten un-compiled SQL, so there may be issues, but you get the drift.&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;This is messy. But that is not the main problem. Quoting from Stack Overflow - &amp;quot;a more important problem is that this solution is notÂ &lt;strong&gt;sargable&lt;/strong&gt;, which means: it bypasses any database indexes onÂ Id1Â andÂ Id2Â that could have been used otherwise. This will perform very very poorly.&amp;quot;&lt;/p&gt;
&lt;p&gt;Note: For an in-depth discussion of Composite Keys and EF ad the general recommended ways of handling them check either this &lt;a href=&quot;https://stackoverflow.com/questions/26198860/entityframework-contains-query-of-composite-key/26199792&quot;&gt;Stack Overflow question&lt;/a&gt; or this &lt;a href=&quot;https://www.codesd.com/item/entityframework-contains-a-composite-key-query.html&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Enter the SARG !&lt;/h2&gt;
&lt;p&gt;I had encountered this SARG before, but had forgotten the details of it, so had to look it up. Turns out SARGability is a big deal in SQL server and other RDBMSs.&lt;/p&gt;
&lt;p&gt;So what is &lt;strong&gt;SARG&lt;/strong&gt;?
Itâ€™s an acronym word derived from Search and Argument. An alternative usage is &lt;strong&gt;SARGable&lt;/strong&gt;, which means &lt;em&gt;&apos;Search Argument Able&apos;&lt;/em&gt;. As per WikipediaÂ &lt;strong&gt;SARGable&lt;/strong&gt;Â is defined as &lt;em&gt;&apos;In relational databases, a condition (or predicate) in a query is said to beÂ sargableÂ if the DBMS engine can take advantage of an index to speed up the execution of the query&apos;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A technical definition would be &lt;em&gt;&apos;Sargability is the query&apos;s ability to traverse the b-tree index using the binary search method that relies on half-set elimination for the sorted items array&lt;/em&gt;. In SQL, it would be displayed on the execution plan as a &amp;quot;index seek&amp;quot;.&apos;&lt;/p&gt;
&lt;h3&gt;Now why is this important?&lt;/h3&gt;
&lt;p&gt;Because indexes are key to fast and efficient RDBMS queries and stored procedures. &lt;small&gt; &lt;em&gt;Note:&lt;/em&gt; At this point, if you are not interested in fast and efficient database queries, you can now go read something else. For the rest however, hopefully this will help.&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Databases have &lt;strong&gt;Index&lt;/strong&gt;es. In fact so important and ubiquitous are database indexes that RDBMSs create all manner of indexes automatically for you. Primary Keys, Foreign Keys, Unique contraints - all are indexes created silently behind the scenes in any respectable database. However, indexes are no good if your query cannot take advantage of them. That is where SARGability comes in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;em&gt;IMPORTANT NOTE - A non sargable query will not use your indexes.&lt;/em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Instead it will fall back to  index scans and table scans, for indexed and non-indexed tables respectively, instead of index seeks, and it will do so all of the time, destroying the entire purpose of indexes and our goal of fast efficient queries.&lt;/p&gt;
&lt;p&gt;To understand why this happens, we will look at a simple but oft-repeated SQL need. Consider a table called Customer with an primary key fields - &lt;strong&gt;Id&lt;/strong&gt;, and a &lt;strong&gt;LastName&lt;/strong&gt; varchar column with an index on it. &lt;em&gt;We need to retrieve Customers whose last name is Smith&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Here are two simple queries which can do this and their query plans&lt;/p&gt;
&lt;h4&gt;Option 1&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1. Select * from Customer where LastName = &apos;Smith&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/posts/sarg-query-plan-seek.png&quot; alt=&quot;Option 1&quot; /&gt;.&lt;/p&gt;
&lt;h4&gt;Option 2&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;2. Select * from Customer where Upper(LastName) = &apos;SMITH&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/posts/sarg-query-plan-scan.png&quot; alt=&quot;Option 1&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;See what happened? The query plan changed from &lt;em&gt;index &lt;strong&gt;seek&lt;/strong&gt;&lt;/em&gt; to &lt;em&gt;index &lt;strong&gt;scan&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this case my table had two columns, with indexes defined on each. A primary key on &lt;em&gt;Id&lt;/em&gt;, a non-primary index on &lt;em&gt;LastName&lt;/em&gt; and still we go from index seek to index scan.&lt;/p&gt;
&lt;h3&gt;Why does this happen? A tale Of B-Trees and Database Indexes.&lt;/h3&gt;
&lt;p&gt;A database is basically like a giant dictionary, with its pages shuffled. The index at the back lists the start word per page number, and each page is numbered. Moreover, more pages are being continuously added or removed, so the index at the back needs to be constantly maintained. But there is an extra twist. The index pages are themselves jumbled. So while the index keeps track of whats where in the main pages, we have to keep track of the index pages itself.  The database depends on keeping rows in logical order by virtue of its keys, separate from physical order. However, that does not mean that all of those rows are in the same contiguous order on disc. The database also depends on keeping its physical order separate from its logical order. This is essentially what a RDBMS &lt;strong&gt;index&lt;/strong&gt; does. The mechanism used to implement all of this consists of a doubly linked list, a B-Tree and a fetch process.&lt;/p&gt;
&lt;h4&gt;Doubly linked list&lt;/h4&gt;
&lt;p&gt;The logical sequence is established between leaf index nodes through a doubly linked list. Each node points to a database block and to its previous and next block. Each page stores as many index entries as possible. Thus there is two layer ordering, across nodes followed by within nodes. Note the sequential entries in an index leaf node has no guarantee of sequential entries on disk or on database blocks. They could be pointing to different areas of physical storage.&lt;/p&gt;
&lt;h4&gt;B-Tree - the Balanced Tree in DBMS.&lt;/h4&gt;
&lt;p&gt;Now for the operating system each page is in random order on disk. To find the database block pages, the database uses what is called as B-Tree, or Balanced Tree. (No, it is not binary tree; At the least, its an n-ary tree, with n variable). In a B-Tree, each branch node consists of the starting entries of the set of leaf nodes it covers. Since it is also a database block in itself, it is quite large and in sorted order of leaf node entries. Thus each entry in each branch node has the starting value(s, if composite key) of the indexed column of the leaf node and the leaf node&apos;s location on disk. An index B-Tree is initially built in such a way that all leaf nodes are covered by a branch node. New layers of branches are created until a branch contains enough information to enable traversing down to the leaf node with the desired index column values and fits in a single database page block. This is constantly maintained. A characteristic of this B-Tree is that all elements can be accessed with the same number of steps, and the depth can change based on the size of the database page block being used by the database to store data.&lt;/p&gt;
&lt;h4&gt;The fetch.&lt;/h4&gt;
&lt;p&gt;There are a few different ways for a database to approach fetching the data.&lt;/p&gt;
&lt;h5&gt;The index seek.&lt;/h5&gt;
&lt;p&gt;The act of reaching for a row data in a particular table based on a given index i.e, you are given the column values to search for, involves the following steps. The database looks up the index and its root page, traverses the page until it finds a branch node whose range covers these keys values, goes deeper one layer, recursively does the same until it finds the leaf node which contains the given keys. Once the leaf node is found, it is iterated/searched until the specific row/key values are reached. This key points to the database block which stores the table data on the heap. Now corresponding data can be fetched if needed, since it depends on the query.&lt;/p&gt;
&lt;h5&gt;The index scan.&lt;/h5&gt;
&lt;p&gt;Index Scan is nothing but scanning on the data pages from the first page to the last page. If there is an index on a table, and if the query is touching a larger amount of data, which means the query is retrieving more than 50 percent or 90 percent of the data, and then the optimizer would just scan all the data pages to retrieve the data rows.&lt;/p&gt;
&lt;h5&gt;The table scan.&lt;/h5&gt;
&lt;p&gt;If there is no index, then you might see a Table Scan in the execution plan. A Table scan touches all database blocks and rows for the table as it doesn&apos;t have the aid of an index&apos;s B-Tree.&lt;/p&gt;
&lt;h3&gt;So, why &apos;index seek&apos; to &apos;index scan&apos;?&lt;/h3&gt;
&lt;p&gt;So, coming back to the question - &lt;em&gt;Why did the query plan change from index &lt;strong&gt;seek&lt;/strong&gt; to index &lt;strong&gt;scan&lt;/strong&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Well, because, in our case the index key was defined on column &apos;LastName&apos;. So the index had the key &apos;Smith&apos; somewhere - root, branch or leaf node. However, &apos;SMITH&apos; was not a value which was present in the index. So a full scan of all table rows, using the index on LastName would be needed, as the Sql optimiser cannot be sure of the data until the string is manually compared, for which it needs to physically read in the table row data from the database block on disk.&lt;/p&gt;
&lt;p&gt;In real world scenarios, with multiple tables, joins and complex clauses and functions, a predicate (where clause) or join condition can easily go from a index seek to an index scan to a table scan.&lt;/p&gt;
&lt;p&gt;SARGability thus greatly determines the choice of indexing &lt;em&gt;used&lt;/em&gt; , &lt;strong&gt;OR&lt;/strong&gt; &lt;em&gt;not used&lt;/em&gt;, and has a direct impact on the query execution time.&lt;/p&gt;
&lt;h2&gt;Breaking SARGability and how to fix.&lt;/h2&gt;
&lt;p&gt;Many scenarios break SARGability. General thumbrules of breaking SARGability and things not to do include -&lt;/p&gt;
&lt;h3&gt;Implicit/explicit datatype conversion before comparision&lt;/h3&gt;
&lt;p&gt;Given that column B is defined as a varchar.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Select * from F WHERE B = 1 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is not sargable. Do the following instead.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Select * from F WHERE B = &apos;1&apos; 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Applying functions/operations to indexed columns before comparisions or joins.&lt;/h3&gt;
&lt;p&gt;In the following case,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Select * from F where DateDiff(day, ODate, GetDate()) &amp;gt; 7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is bad. The SQL optimizer can&apos;t use an index on the field, even if one exists. It will literally have to evaluate this function for every row of the table. The better option is to have only the ODate column in where clause. A rewrite would look like this -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Select * from F where ODate &amp;gt;= DateAdd(day, -7, Cast(GetDate() as Date))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basically, If possible, use functions in reverse. Instead of &lt;code&gt;WHERE FUNCTION(Field) = &apos;BLAH&apos;&lt;/code&gt;, do &lt;code&gt;WHERE Field = INVERSE_FUNCTION(&apos;BLAH&apos;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Other examples -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Bad: Select ... WHERE isNull(FullName,&apos;Ed Jones&apos;) = &apos;Ed Jones&apos;
Fixed: Select ... WHERE ((FullName = &apos;Ed Jones&apos;) OR (FullName IS NULL)) (Note that this is bad for other reasons - not being able to cache a query plan).

Bad: Select ... WHERE SUBSTRING(DealerName,4) = &apos;Ford&apos;
Fixed: Select ... WHERE DealerName Like &apos;Ford%&apos;

Bad: SELECT ...WHERE Number + 0 = 42
Fixed: SELECT ...WHERE Number  = 42 - 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;small&gt;Note: Read somewhere that An explicit CAST of a DATE column to DATETIME still leaves the predicate SARGable. This is an exception thatâ€™s been specifically coded into the optimiser. Not verified though.&lt;/small&gt;&lt;/p&gt;
&lt;h3&gt;Wild-cards&lt;/h3&gt;
&lt;p&gt;Leading on from the substring query in the list just above, do you see the wildcard in&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Select ... WHERE Manufacturer Like &apos;Ford%&apos;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that the wildcard is &lt;strong&gt;NOT&lt;/strong&gt; a leading wildcard&lt;/em&gt;. Leading wildcards are bad. The following is bad.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Select ... WHERE Manufacturer Like &apos;%ord%&apos;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This basically turns the query into a full-text search query with an index scan, or worse, a table scan.&lt;/p&gt;
&lt;h4&gt;Tip : Indexes on computed columns.&lt;/h4&gt;
&lt;p&gt;Sometimes the use of functions and so on is unavoidable. The general solution is to add a calculated column for the field value returned by the function and then create a non-clustered index for that calculated column.&lt;/p&gt;
&lt;p&gt;Note that this computed column does not need to be persisted. Consider the query&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT LastName  FROM Customers where LEFT(LastName,1)=&apos;K&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, add a computed column &lt;em&gt;&apos;LastNameStartsWith&apos;&lt;/em&gt; to hold &lt;code&gt;Left(LastName, 1)&lt;/code&gt;, and add a non-clustered index on &lt;em&gt;&apos;LastNameStartWith&apos;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Be aware that this is a double edged sword&lt;/em&gt;&lt;/strong&gt; however, as you may end up creating lots of indexes for every use combination. Each new index needs to be maintained continuously by the database and ultimately may be counter effective. Be discreet.&lt;/p&gt;
&lt;p&gt;Hope this helped. Good luck with the queries.&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/10853774/is-this-date-comparison-condition-sarg-able-in-sql&quot;&gt;Sargable date comparisions&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/26198860/entityframework-contains-query-of-composite-key/42412401&quot;&gt;Composite Keys and Contains&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/799584/what-makes-a-sql-statement-sargable&quot;&gt;SQl Sargability&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://dba.stackexchange.com/questions/132437/sargable-where-clause-for-two-date-columns&quot;&gt;Sargable where clauses&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://tsqlninja.wordpress.com/2012/05/29/join-me-inverting-joins-to-maintain-sargability/&quot;&gt;Inverting Joins&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://blogs.lobsterpot.com.au/2010/01/22/sargable-functions-in-sql-server/&quot;&gt;Sql Server Sargable functions&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blogs.msmvps.com/robfarley/2010/02/01/a-case-study-in-sargability/&quot;&gt;Case study in Sargability&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>JavaScript - The Good Parts vs The Definitive Guide</title>
      <link>https://insen.github.io/blog/2017/11/01/JS-Good-Vs-Definitive/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/11/01/JS-Good-Vs-Definitive/</guid>
      <description>&lt;p&gt;It&apos;s said that some pictures say more than a thousand words. Case in point below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/posts/Javascript-GoodPartsVsDefinitiveGuide.png&quot; alt=&quot;JavaScript - Good vs Definitive&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer : Your inferences are your own.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Image source is &lt;a href=&quot;https://medium.com/@basarat/typescript-won-a4e0dfde4b08&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Connecting to Windows Azure Storage through PowerShell</title>
      <link>https://insen.github.io/blog/2017/10/16/Powershell-Azure-Storage/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/10/16/Powershell-Azure-Storage/</guid>
      <description>&lt;p&gt;I came across this paper describing the Windows Azure Storage design &lt;a href=&quot;&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And with that, I stopped thinking about Windows Azure Storage as as an extended file system and started thinking of it as a distributed, highly available, noSql persistence layer provided by the Azure platform.&lt;/p&gt;
&lt;p&gt;That changes things quite a bit.&lt;/p&gt;
&lt;h2&gt;Windows Azure Storage - &lt;em&gt;not an extended file system&lt;/em&gt;.&lt;/h2&gt;
&lt;p&gt;It provides all three of Consistency, Availability and Partition tolerance, which apparently violates CAP, but has actually been made possible through some heavy lifting as detailed in the paper.&lt;/p&gt;
&lt;p&gt;It offers five different sorts of distributed data services&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blobs
&lt;ul&gt;
&lt;li&gt;Block blob &lt;em&gt;for streaming data&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Page blob &lt;em&gt;for Random Access data&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Append blob &lt;em&gt;for Append-only data&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Table &lt;em&gt;for unstructured tabular data&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Queues &lt;em&gt;for sequential processing&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Disks &lt;em&gt;are usually usually VHDs for IAAS&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Files - &lt;em&gt;are usually premium NAS for lift-and-shift&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Only problem, is, all of this through the Azure Portal is a lot of clicks. Now, all Azure services offer REST apis, and PowerShell access. So, let&apos;s see if all those clicks can be mutated into key-strokes, logging into and accessing Windows Azure Storage through PowerShell commands.&lt;/p&gt;
&lt;h3&gt;A short but important note.&lt;/h3&gt;
&lt;p&gt;After a while of fiddling around with Azure PowerShell commands, you will find that there are two versions of almost everything.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Login-AzureAccount&lt;/code&gt; vs &lt;code&gt;Login-AzureRmAccount&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Get-AzureStorageAccount&lt;/code&gt; vs &lt;code&gt;Get-AzureRmStorageAccount&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This non &lt;code&gt;Rm&lt;/code&gt; and &lt;code&gt;Rm&lt;/code&gt; options indicate Azure operational models before and after there was a Azure concept called &lt;em&gt;resource group&lt;/em&gt;. A &lt;em&gt;resource group&lt;/em&gt; is a logical grouping of Azure physical services - viz. storage, VMs, etc.&lt;/p&gt;
&lt;p&gt;To read more, look &lt;a href=&quot;https://blogs.technet.microsoft.com/meamcs/2016/12/22/difference-between-azure-service-manager-and-azure-resource-manager/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://azuredepot.com/2017/01/02/azure-login-options/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our case, we need to be conscious that wherever there is a choice, we need to go with the &lt;code&gt;Rm&lt;/code&gt; versions.&lt;/p&gt;
&lt;h2&gt;Setting Up PowerShell for Windows Azure&lt;/h2&gt;
&lt;p&gt;For a fresh machine, we need to have the &lt;code&gt;AzureRM&lt;/code&gt; package installed. On an elevated (Administrator) prompt&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Verify you have &lt;code&gt;PowerShellGet&lt;/code&gt; Module installed.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Get-Module PowerShellGet -list | Select-Object Name,Version,Path&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install Azure RM&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Install-Module AzureRM&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you have a previous version of Azure PowerShell installed that includes the Service Management module, you may receive an error. In that case, do this.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Install-Module AzureRM -AllowClobber&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To verify &lt;code&gt;AzureRM&lt;/code&gt; was installed run&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Get-Module -ListAvailable -Name AzureRM&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Azure Credentials for PowerShell Session&lt;/h2&gt;
&lt;p&gt;Close the current PS prompt and open a non-elevated one.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Login to Azure&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Login-AzureRmAccount&lt;/code&gt; - This now does the Oauth dance and takes you to a login screen, takes your input, authenticates and returns to the prompt.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add Azure Account to PowerShell&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Add-AzureAccount&lt;/code&gt; - This should show you subscription details on command prompt&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Id/Type/Subscriptions/Tenants

id@mail.com/User/d81de17a-cc30-4bee-a874-27d1eea6062/{aea858f0-512d-4649-8228-d78fd9ef3c76}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set your current subscription as the default.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Select-AzureSubscription -Current -SubscriptionId &amp;quot;your-subscription-id&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To verify, run&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Get-AzureRmContext&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And that is it. We are all set.&lt;/p&gt;
&lt;h2&gt;Working with Azure Storage Accounts&lt;/h2&gt;
&lt;h3&gt;Create&lt;/h3&gt;
&lt;p&gt;To create a Azure Storage Account&lt;/p&gt;
&lt;p&gt;&lt;code&gt;New-AzureRmStorageAccount -ResourceGroupName $your-resource-group -Name &amp;quot;your-storage-acc-name&amp;quot; -Location $yourDefinedlocation -SkuName Standard_LRS -Kind Storage -EnableEncryptionService Blob&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;View&lt;/h3&gt;
&lt;p&gt;To access all the storage accounts for your MSDN account, try this -&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Get-AzureRmStorageAccount -ResourceGroupName &amp;quot;your-resource-group&amp;quot; -AccountName &amp;quot;account-name&amp;quot; | Select StorageAccountName&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see your StorageAccountName on your console, including the one created above.&lt;/p&gt;
&lt;h3&gt;Delete&lt;/h3&gt;
&lt;p&gt;To remove a storage account, the following applies -&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Remove-AzureRmStorageAccount -ResourceGroupName &amp;quot;your-resource-group&amp;quot; -AccountName &amp;quot;your-storage-account&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;From here on, PowerShell commands should enable you to do almost anything with an Azure Storage Account you can do over REST or .NET libraries. All we need to do is explore.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Words - The Road To Wisdom</title>
      <link>https://insen.github.io/blog/2017/10/12/The-Road-To-Wisdom/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/10/12/The-Road-To-Wisdom/</guid>
      <description>&lt;p&gt;Over my life I have always collected words, but over time, they keep getting lost. Now that this blog has gotten going, that&apos;s one place I have to both store and share.&lt;/p&gt;
&lt;p&gt;All backed by Github.&lt;/p&gt;
&lt;p&gt;Came across something marvellous called &lt;em&gt;&apos;grooks&apos;&lt;/em&gt; by this Danish guy called Piet Hein. A sample or two below.&lt;/p&gt;
&lt;p&gt;Here&apos;s one&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The road to wisdom? -- Well, it&apos;s plain
    and simple to express:
        Err
        and err
        and err again
        but less
        and less
        and less.

            --Piet Hein (Grooks)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And another&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;We must expect posterity
    to view with some asperity
    the marvels and the wonders
        we&apos;re passing on to it;
    but it should change its attitude
    to one of heartfelt gratitude
    when thinking of the blunders
        we didn&apos;t quite commit.

        --Piet Hein (Grooks)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;from &lt;a href=&quot;http://leptonica.com/cachedpages/grooks/grooks.html&quot;&gt;leptonica&lt;/a&gt; - a site on particle physics and image processing, of all places.&lt;/p&gt;
&lt;p&gt;Click on through for more &lt;a href=&quot;http://www.archimedes-lab.org/grooks.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>W3C LinkChecker &amp; Perl - Check broken links before commit</title>
      <link>https://insen.github.io/blog/2017/10/10/Blogging-Perl-DevOps/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/10/10/Blogging-Perl-DevOps/</guid>
      <description>&lt;p&gt;I finally encounter Perl.&lt;/p&gt;
&lt;p&gt;Now it started simple. And turned into a blog post. That pretty much sums up many aspects of software engineering, but digress let us not.&lt;/p&gt;
&lt;p&gt;I was having a problem with my still nascent blog.&lt;/p&gt;
&lt;p&gt;I am figuring out a lot more CSS than I thought I ever would and I&apos;m still tweaking and turning, so churn is high. Therefore links and cross references are breaking every now and then.&lt;/p&gt;
&lt;p&gt;This, however, is not acceptable in production.&lt;/p&gt;
&lt;p&gt;Moreover, since I have been dealing with devOps a bit lately, let us try some devOps in blogging. Automated link checking sounds nice.&lt;/p&gt;
&lt;h3&gt;Enter W3C Link Checker.&lt;/h3&gt;
&lt;p&gt;A link checker should be freely available. Yep. Many are. Including a highly recommended one from the world wide web consortium, no less.&lt;/p&gt;
&lt;p&gt;Copyrighted From 1999.&lt;/p&gt;
&lt;p&gt;With both online and commmand-line options. &lt;em&gt;someone come and tell me again that SAAS is new?!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Written and packaged in Perl  !&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Okay, Perl !&lt;/h3&gt;
&lt;p&gt;Wikipedia says&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;It has been nicknamed &amp;quot;the Swiss Army chainsaw of scripting languages&amp;quot; because of its flexibility and power,[18] and also its ugliness.[19]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This ugliness and power combined of Perl is something I have repeatedly encountered in the blogosphere over the years. At some period of time I had also read a good deal of a blog called &lt;a href=&quot;http://www.sidhe.org/%7Edan/blog/&quot;&gt;&lt;em&gt;squawks of the parrot&lt;/em&gt;&lt;/a&gt;. And so I have avoided Perl.&lt;/p&gt;
&lt;p&gt;And then there is Larry Wall - creator of Perl. From the last line of wikipedia on Larry Wall.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The official Perl documentation states that:
	Larry is always by definition right about how Perl should behave. This means he has final veto power on the core functionality.
	Larry is allowed to change his mind about any matter at a later date, regardless of whether he previously invoked Rule 1.
	Got that? Larry is always right, even when he was wrong.[10]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, the tool source comes zipped as &lt;code&gt;tar.gz&lt;/code&gt;. That alreadys tell me how windows friendly it is likely to be.&lt;/p&gt;
&lt;h3&gt;Up and running&lt;/h3&gt;
&lt;p&gt;Ok, perl it is. So what is the goal here ?&lt;/p&gt;
&lt;p&gt;Get a local version of the W3C Link Checker running so that I could test all the links in my blog before I publish/push them to Github.&lt;/p&gt;
&lt;h4&gt;Download Perl.&lt;/h4&gt;
&lt;p&gt;There are two windows distros - Active Perl and Strawberry. very &lt;em&gt;unix-y&lt;/em&gt;. Both recommended by &lt;a href=&quot;http://www.perl.org/&quot;&gt;http://www.perl.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I went with strawberry as it is also available on chocolatey.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;choco install strawberryperl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, gets installed at &lt;code&gt;C:\Windows\Strawberry&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;Clone W3C Link Checker source.&lt;/h4&gt;
&lt;p&gt;Luckily we don&apos;t have to eyeball tarballs. The source is on &lt;a href=&quot;https://github.com/w3c&quot;&gt;Github&lt;/a&gt; with docs &lt;a href=&quot;https://github.com/w3c/link-checker&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main instructions are to execute the following commands from root directory the unzipped tarball package or the cloned source.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;#if you have cpanminus installed
cpanm --installdeps .
perl Makefile.PL
make
make test
make install # as root unless you are using local::lib
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;CPAN, perl and make&lt;/h4&gt;
&lt;p&gt;What &lt;em&gt;is&lt;/em&gt; CPAN? CPAN is basically like a Maven central for Java or Nuget central for .NET. It&apos;s one of the major perl eco-system&apos;s package repositories. &lt;code&gt;cpan&lt;/code&gt; and &lt;code&gt;cpanm&lt;/code&gt; are command line tools that enable interaction with this repo.&lt;/p&gt;
&lt;p&gt;Do I have CPAN? Apparently yes, after some reading around! Strawberry Perl comes with its own CPAN client. &lt;a href=&quot;https://stackoverflow.com/questions/6643939/installing-modules-using-strawberry-perl&quot;&gt;Stack Overflow&lt;/a&gt; says this -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;you can still use ppm, but it is not recommended. Run CPAN client from the Strawberry Perl or Strawberry Perl (64-bit), sub folder Tools, entry in the Start menu.

Type install Module::Name there.

On Windows 7, Start Menu &amp;gt; Strawberry Perl &amp;gt; Tools &amp;gt; CPAN Client 
On Windows 8.1, Start&amp;gt;Cpan Client 
On Windows 10, Start Menu &amp;gt; All Apps &amp;gt; Strawberry Perl &amp;gt; CPAN Client 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Add strawberryperl/c/bin to &apos;%PATH%&apos;&lt;/h4&gt;
&lt;p&gt;Combining the two steps above, we should be good. So on an elevated Powershell, we navigate to the root of our unzipped tarball and run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;cpanm --installdeps .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No we are not good.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&apos;cpanm&apos; is not recognized as an internal or external command, operable program or batch file - Windows 7.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Forgot the &lt;code&gt;%PATH%&lt;/code&gt; environment variable ! Add &lt;code&gt;full/path/to/strawberry/c/bin&lt;/code&gt; to &lt;code&gt;%PATH%&lt;/code&gt;. Restart powershell prompt and re-execute same command.&lt;/p&gt;
&lt;h4&gt;makefile&lt;/h4&gt;
&lt;p&gt;This one goes without hiccups.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;perl Makefile.PL
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&apos;make&apos;&lt;/h4&gt;
&lt;p&gt;And then, I am at &lt;em&gt;make&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&apos;make&apos; is not recognized as an internal or external command, operable program or batch file - Windows 7.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking through the files in strawberry, you find a &lt;code&gt;gmake.exe&lt;/code&gt; and a &lt;code&gt;dmake.exe&lt;/code&gt;. A quick google for gmake.exe shows this as first result&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Make for Windows - GnuWin32
gnuwin32.sourceforge.net/packages/make.htm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like we need the &lt;code&gt;gmake&lt;/code&gt; command. The following works&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;gmake
gmake test
qmake install # as root unless you are using local::lib
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command shell shows the following&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Installing C:\STRAWB~1\perl\site\lib\W3C\LinkChecker.pm
Installing C:\STRAWB~1\perl\site\bin\checklink
Installing C:\STRAWB~1\perl\site\bin\checklink.bat
Appending installation info to C:\STRAWB~1\perl\lib/perllocal.pod
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the batch file is ready at &lt;code&gt;full\path\to\strawberry\perl\site\bin&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The trick I missed here was that there is a copy of these files in the unzipped tarball folder itself, so it looks like all I needed was &lt;code&gt;gmake&lt;/code&gt;, not &lt;code&gt;gmake install&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;p.s: 1 test fails. Ignored.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Invoke linkChecker from command line.&lt;/h3&gt;
&lt;p&gt;The docs are &lt;a href=&quot;https://wummel.github.io/linkchecker/man1/linkchecker.1.html&quot;&gt;here&lt;/a&gt;, And as per the docs, that&apos;s trivial, execute from a command prompt. OR is it?&lt;/p&gt;
&lt;p&gt;when you run the following&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;$&amp;gt; full/path/to/checklink.bat uri
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You get this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;-T&amp;quot; is on the #! line, it must also be used on the command line.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem here is that the &lt;code&gt;checklink&lt;/code&gt; and the &lt;code&gt;checklink.bat&lt;/code&gt; files contain a line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;#!/usr/bin/perl -wT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this file path does not exist in windows. Replace with &lt;code&gt;full/path/to/strawberry/perl&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note I removed the -wT as well. The &apos;T&apos; refers to something called tainted mode and is safer, but more restrictive as well.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Run the command again. Linkchecker should now give you a HTTP status code based report. There are a few problems though. Some valid links fail, viz.  &lt;code&gt;https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Well, since I intend to inspect the failures manually, my goal here is achieved. I can now run the linkchecker against &lt;code&gt;http://127.0.0.0:8080/&lt;/code&gt; and confirm that all my links are fine.&lt;/p&gt;
&lt;h4&gt;Next Steps&lt;/h4&gt;
&lt;p&gt;Programmatic detection of fails.
Explore exclusion flag in command options.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Running this against HTTPS involves more work. lots more. Primarily around installing LWP::Protocol::Https through CPAN and ensuring all dependencies are set up right ! In my case, I did some of the work, but I haven&apos;t gotten all https links to work. Some still keep failing, sometimes intermittently. Primary issue encountered is this&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://www.linkedin.com/in/nilsengupta/
Line: 127
Code: 500 Can&apos;t locate object method &amp;quot;new&amp;quot; via package &amp;quot;LWP::Protocol::https::Socket&amp;quot;
To do: This is a server side problem. Check the URI.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Blogging - Loose ends and curious things when using Pretzel and the Jekyll-Hyde template.</title>
      <link>https://insen.github.io/blog/2017/10/04/Pretzel-Curious-Things/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/10/04/Pretzel-Curious-Things/</guid>
      <description>&lt;p&gt;Pretzel blogging turned up some normal headscratchers, and some very curious ones.&lt;/p&gt;
&lt;h4&gt;Paginator&lt;/h4&gt;
&lt;p&gt;Couldn&apos;t get paginator to work. Well, it works now. Turns out you have to add&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;
paginate:   number_of_posts_in_a_page
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;at the top of your &lt;code&gt;index.html&lt;/code&gt; in the &lt;code&gt;yml&lt;/code&gt; frontmatter.&lt;/p&gt;
&lt;h4&gt;Escaping Liquid&lt;/h4&gt;
&lt;p&gt;I couldn&apos;t escape the characters &lt;code&gt;{{&lt;/code&gt; or the characters &lt;code&gt;{%&lt;/code&gt; in any fashion.&lt;/p&gt;
&lt;p&gt;You can, if you put it like this &lt;code&gt;{{&amp;quot;{{&amp;quot;}}&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Landing page to show list only.&lt;/h4&gt;
&lt;p&gt;The landing page was updated to show list of posts. Change is in &lt;code&gt;index.html&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Comment out the following line &lt;code&gt;{{ post.content }}&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Css font-size reduction&lt;/h4&gt;
&lt;p&gt;The basic font size was too large. Changes in &lt;code&gt;hyde.css&lt;/code&gt;. The &lt;code&gt;font-size&lt;/code&gt; was reduced by &lt;code&gt;2px&lt;/code&gt;. The section below is the relevant section. All other fonts are in &lt;code&gt;em&lt;/code&gt; with relation to these two. So this is a across all devices and form-factors thing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;
@media (min-width: 48em) {
    html {
        font-size: 14px;
    }
}
@media (min-width: 58em) {
    html {
        font-size: 18px;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Sidebar profile image responsive display and alignment&lt;/h4&gt;
&lt;p&gt;Adding a profile picture to the sidebar was easy. Getting it to vertically align centers with the rest of the sidebar and do so in all form-factors/view-ports was challenging. Ended up modifying &lt;code&gt;hyde.css&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;pre&amp;gt;&amp;lt;code class=&amp;quot;language-css&amp;quot;&amp;gt;
    .sidebar-about img { margin : 0.1rem; height: 80%; width: 100%; vertical-align: text-bottom; justify-content: center;}
&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&apos;Related Post&apos; working&lt;/h4&gt;
&lt;p&gt;&apos;Related Posts&apos; section was not working. That is because the Pretzel site object model which exists is different from that expected by Hyde.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&amp;quot;related&amp;quot;&amp;gt;
  &amp;lt;h2&amp;gt;Related Posts&amp;lt;/h2&amp;gt;
  &amp;lt;ul class=&amp;quot;related-posts&amp;quot;&amp;gt;
    {% for post in site.posts limit:6 %}
      {% if (page.title != post.title and post.tags contains page.tags[0]) %}
        &amp;lt;li&amp;gt;
          &amp;lt;h3&amp;gt;
            &amp;lt;a href=&amp;quot;{{ post.url | prepend: site.url }}&amp;quot;&amp;gt;
              {{ post.title }}
              &amp;lt;small&amp;gt;{{ post.date | date_to_string }}&amp;lt;/small&amp;gt;
            &amp;lt;/a&amp;gt;
          &amp;lt;/h3&amp;gt;
        &amp;lt;/li&amp;gt;
      {% endif %}
    {% endfor %}
  &amp;lt;/ul&amp;gt;
&amp;lt;/div&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the part where &lt;code&gt;site.posts limit:6&lt;/code&gt;, the code contained &lt;code&gt;site.related_posts&lt;/code&gt;. Pretzel does not expose a property called &lt;code&gt;related_posts&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&apos;FontAwesome&apos; social icons&lt;/h4&gt;
&lt;p&gt;FontAwesome is a font family with awesome icons. I wanted these icons for the social links, so had to integrate FontAwesome. Following steps apply&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the fontawesome fonts and put them in &lt;code&gt;root/public/fonts&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Download &lt;code&gt;fontawesome.css&lt;/code&gt; and add a stylesheet link in &lt;code&gt;head.html&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Add the following classes to &lt;code&gt;hyde.css&lt;/code&gt;&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;
    .fa::before {font: normal normal normal 14px/1 FontAwesome}
    .linkedin::before { content: &quot;\f08c&quot;; font-size: 1.5em}
    .github::before { content: &quot;\f09b&quot;; font-size: 1.5em}
    .rss::before { content: &quot;\f143&quot;; font-size: 1.5em}    
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Add the classes to the relevant social &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tags in the &lt;code&gt;sidebar.html&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And in general, all navigation, posts and rss feed link now working ok.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GOF - The hidden pattern.</title>
      <link>https://insen.github.io/blog/2017/10/03/GOF-The-24th-Pattern/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/10/03/GOF-The-24th-Pattern/</guid>
      <description>&lt;p&gt;The Gang-Of-Four book has 24 patterns. And it only took me only about 12 years or so to get it, considering that I first encountered the book in 2005.&lt;/p&gt;
&lt;p&gt;I know, I know. I can hear the collective tirade of protest.&lt;/p&gt;
&lt;p&gt;All the analysis and the literature says 23.&lt;/p&gt;
&lt;p&gt;Anyone who can count to 23 says 23.&lt;/p&gt;
&lt;p&gt;The number of pattern descriptions in the book are 23.&lt;/p&gt;
&lt;p&gt;In this case at least, as far as common knowledge goes, 23 is the answer, not 42.&lt;/p&gt;
&lt;p&gt;Yet I will argue. There is another pattern in there. A pattern that is repeated over and over. A pattern that embeds yourself in your psyche and you use it unconsciously for several years until it hits you.&lt;/p&gt;
&lt;p&gt;You disagree, eh?  Ok then, we&apos;ll try a different tack.&lt;/p&gt;
&lt;h2&gt;Pattern No. 24. An oblique introduction.&lt;/h2&gt;
&lt;p&gt;Are you familiar with the word &apos;concept&apos;? &lt;em&gt;Yes. Good. Thank you. A no here would have made it really difficult to proceed&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So you have maybe a concept or two of your own. &lt;em&gt;Yes, Even better&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now here&apos;s the kicker - explain your concept to me, in writing. (&lt;em&gt;It doesn&apos;t have to be technical even. It could be in woodworking for all I care&lt;/em&gt;). But try writing down the concept.&lt;/p&gt;
&lt;p&gt;Yes, go right ahead. Get some notes, stickers, postits, onenotes/evernotes, mindmaps and just write out the outline of your explanation. I can wait. See I have coffee, blogs, white-noise. I am all good with waiting a bit.&lt;/p&gt;
&lt;p&gt;Writing down a concept is not an uncommon activity in the software engineering line of business. We are asked to explain concepts all of the while, in writing - RFP submissions, software design documents, new initiative proposals, POC summaries. I have done my bit.&lt;/p&gt;
&lt;p&gt;Recently I did one more - A &lt;em&gt;&apos;starting devOps&apos;&lt;/em&gt; analysis. And it was while doing this that I also started to think about &apos;&lt;em&gt;how to explain&lt;/em&gt;&apos; such a thing to someone else. And I found that everytime I need to explain a concept, I tend to use the same patterns to do so. It was a curious co-incidence that design pattern fever was running rather high at the workplace at that period, and recurring discussions were happening around this famous book and its contents.&lt;/p&gt;
&lt;h2&gt;Explaining yourself, preferably clearly.&lt;/h2&gt;
&lt;p&gt;So a quick look at what I do when writing down a concept?&lt;/p&gt;
&lt;h3&gt;Step 1 - Motivation&lt;/h3&gt;
&lt;p&gt;This part defines the &lt;em&gt;motivations and reasons&lt;/em&gt; for the concept. In my case, It has traditionally involved consideration and listing down of the following&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The driving forces&lt;/li&gt;
&lt;li&gt;The constraining forces&lt;/li&gt;
&lt;li&gt;The goals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 2 - Drivers and Constraints&lt;/h3&gt;
&lt;p&gt;Given the &lt;em&gt;drivers, constraints and goals&lt;/em&gt;, I now proceed to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An explanation of the general structure of the concept.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 3 - Structure&lt;/h3&gt;
&lt;p&gt;Given the &lt;em&gt;drivers, and the structure&lt;/em&gt;, we now drill down further into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Key stake-holders&lt;/li&gt;
&lt;li&gt;Key components involved in the concept,&lt;/li&gt;
&lt;li&gt;Key interactions between components themselves or with a stakeholder, all being traceable to one or more of the motivations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 4 - Downsides and Limitations&lt;/h3&gt;
&lt;p&gt;The next step is usually to play devils advocate. As such, we make a listing of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assumptions&lt;/li&gt;
&lt;li&gt;Tradeoffs&lt;/li&gt;
&lt;li&gt;Areas of applicability&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 5 - References&lt;/h3&gt;
&lt;p&gt;We are now reaching the close of the explanation. At this point, we should be citing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;li&gt;Documentation for further details.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 6 - Risks, Mitigations and Alternatives&lt;/h3&gt;
&lt;p&gt;No concept paper is complete without a brief note about cases which have a high chance of rendering the current concept invalid and leads us to look for alternative solutions. So we note down the&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Limitations of concept&lt;/li&gt;
&lt;li&gt;Alternative possibilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully still here with me. And not too many objections. Ok.&lt;/p&gt;
&lt;p&gt;Now I will copy a passage from a textbook. The following section is a verbatim copy-paste from the GOF book&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Describing Design Patterns

How do we describe design patterns? Graphical notations, while important and useful, aren&apos;t sufficient. They simply capture the end product of the design process as relationships between classes and objects. To reuse the design, we must also record the decisions, alternatives, and trade-offs that led to it. Concrete examples are important too, because they help you see the design in action.

We describe design patterns using a consistent format. Each pattern is divided into sections according to the following template. The template lends a uniform structure to the information, making design patterns easier to learn, compare, and use.

Pattern Name and Classification
The pattern&apos;s name conveys the essence of the pattern succinctly. A good name is vital, because it will become part of your design vocabulary. The pattern&apos;s classification reflects the scheme we introduce in Section 1.5.

Intent
A short statement that answers the following questions: What does the design pattern do? What is its rationale and intent? What particular design issue or problem does it address?

Also Known As
Other well-known names for the pattern, if any.

Motivation
A scenario that illustrates a design problem and how the class and object structures in the pattern solve the problem. The scenario will help you understand the more abstract description of the pattern that follows.

Applicability
What are the situations in which the design pattern can be applied? What are examples of poor designs that the pattern can address? How can you recognize these situations?

Structure
A graphical representation of the classes in the pattern using a notation based on the Object Modeling Technique (OMT) [RBP+91]. We also use interaction diagrams [JCJO92, Boo94] to illustrate sequences of requests and collaborations between objects. Appendix B describes these notations in detail.

Participants
The classes and/or objects participating in the design pattern and their responsibilities.

Collaborations
How the participants collaborate to carry out their responsibilities.

Consequences
How does the pattern support its objectives? What are the trade-offs and results of using the pattern? What aspect of system structure does it let you vary independently?

Implementation
What pitfalls, hints, or techniques should you be aware of when implementing the pattern? Are there language-specific issues?

Sample Code
Code fragments that illustrate how you might implement the pattern in C++ or Smalltalk.

Known Uses
Examples of the pattern found in real systems. We include at least two examples from different domains.

Related Patterns
What design patterns are closely related to this one? What are the important differences? With which other patterns should this one be used?

The appendices provide background information that will help you understand the patterns and the discussions surrounding them. Appendix A is a glossary of terminology we use. We&apos;ve already mentioned Appendix B, which presents the various notations. We&apos;ll also describe aspects of the notations as we introduce them in the upcoming discussions. Finally, Appendix C contains source code for the foundation classes we use in code samples.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;The hidden pattern, finally&lt;/h2&gt;
&lt;p&gt;And here we are. The approach taken to describe the Patterns in the first place.&lt;/p&gt;
&lt;p&gt;It is repeated 23 times in the book. In each chapter, to be precise. If you read the book thoroughly, not only do you understand specific patterns in full, but you learn how to analyse and understand patterns. It is similiar to learning algorithms, but it is &lt;em&gt;NOT just about learning algorithms&lt;/em&gt;, but learning &lt;em&gt;how to analyse them&lt;/em&gt; as well.&lt;/p&gt;
&lt;p&gt;The outline presented here is basically what I fall back to everytime I need to explain a concept. And like patterns itself, I use some of it, all of it, or in combination with something else, but the concept remains.&lt;/p&gt;
&lt;p&gt;Thus, Ladies and Gentlemen, Geeks and Nerds, I give you the GOF Pattern No. 24.&lt;/p&gt;
&lt;h2&gt;The &apos;Concept Elucidation Pattern&apos;.&lt;/h2&gt;
&lt;p&gt;All yours, now.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DevOps - Getting off the blocks.</title>
      <link>https://insen.github.io/blog/2017/09/27/DevOps-Making-A-Start/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/09/27/DevOps-Making-A-Start/</guid>
      <description>&lt;p&gt;DevOps is everywhere these days. As it should be, yes. Yet that doesn&apos;t take anything away from the fact that DevOps is a difficult concept to understand and implement. The principles are fair and easy enough to grasp, but then the &lt;em&gt;where to begin&lt;/em&gt; question comes in, and that is a very difficult question for something as nebulous as DevOps.&lt;/p&gt;
&lt;p&gt;So how does one start being &apos;DevOps&apos;y?  What drives DevOps?&lt;/p&gt;
&lt;h2&gt;Context, Culture, Tools, Change, Automation, Feedback Loops and Collaboration !&lt;/h2&gt;
&lt;p&gt;The above set summarizes the key drivers of DevOps anywhere. Additionally, there are a small but definite set of foundational DevOps practises, which have been proven enablers of DevOps over time.&lt;/p&gt;
&lt;h2&gt;Foundational DevOps Practises&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One touch build - &lt;em&gt;there should be a single command to build and package the tested version of a system.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;One touch deploy - &lt;em&gt;there should be a single command to deploy a tried and tested version to an environment.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Automation - &lt;em&gt;everything repetitive should be automated.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Monitoring Dashboards - &lt;em&gt;all process information, tests, builds, deploys, operations, features, analytics should be monitored and available.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, the practises above together cumulate into a CI/CD pipeline. How sophisticated/refined your CI/CD pipeline is, is one of the key technological measures of a DevOps oriented team&apos;s maturity level.&lt;/p&gt;
&lt;p&gt;So then the question driving a group trying to adopt a DevOps-y approach to software service delivery is, most likely, this&lt;/p&gt;
&lt;h2&gt;Our DevOps Question&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Given our current context, what culture, practises and tools should I adopt which will enable&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;faster change adoption&lt;/strong&gt;?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;more automation&lt;/strong&gt;?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;more shared process information&lt;/strong&gt;?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;more closed feedback loops&lt;/strong&gt;?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Note : Remember this question, We&apos;ll come back to this time and again.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we formulate an answer to the question above, we should have a workable Devops Strategy 101. Let us try an exercise and see.&lt;/p&gt;
&lt;h2&gt;Setting the &apos;&lt;em&gt;Context&lt;/em&gt;&apos;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt; is all about &lt;em&gt;NOT&lt;/em&gt; sacrificing global optima to achieve local optima. &lt;em&gt;But then my junior dev comes up, the one I hired last month, saying he doesn&apos;t know what the ceo knows so he doesn&apos;t have all the context so he can&apos;t code a-la DevOps.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Oops!&lt;/p&gt;
&lt;p&gt;That didn&apos;t sound legit, did it? Cos it wasn&apos;t. In DevOps context itself is context-sensitive, i.e. it means take the biggest picture from where you stand. That&apos;s all you can do anyways.&lt;/p&gt;
&lt;p&gt;Since context is king, we will set a context within which to start out on our DevOps analysis. Assume the following organizational scenario first.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A team trying to provide custom software development and integration services on Azure. A team trying to a create a distributed processing layer for the .NET Stack system. A team working on big-data analytics and data presentation system on Azure. A management team trying to manage all these.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Also assume that all of the teams have various degrees of maturity on the build, test, deploy aspects of their individual systems - so effectively, automated builds, automated tests, and CI/CD exists, but to various degrees. Which is usually the case in diverse organizations.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So the context here is a bunch of &lt;em&gt;software engineers trying to provide system development services on Azure&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Next.&lt;/p&gt;
&lt;h2&gt;Consider &apos;Culture&apos;&lt;/h2&gt;
&lt;p&gt;Now what can we do about Culture here?&lt;/p&gt;
&lt;p&gt;Well, in a software development group, there is one &lt;em&gt;&apos;Culture&apos;&lt;/em&gt; culture, or group culture or organization culture.&lt;/p&gt;
&lt;p&gt;Now this top-level culture is probably not something everybody can start off with. But in every culture, there are sub-cultures. And there are sub-cultures across development tools, eco-systems or resources. &lt;em&gt;(Java vs .Net, anyone?)&lt;/em&gt;. So for a start, let&apos;s just pick a culture by eco-system. Since Azure is common across, let&apos;s pick Azure.&lt;/p&gt;
&lt;p&gt;Enter &lt;em&gt;&lt;strong&gt;Azure Culture&lt;/strong&gt;&lt;/em&gt;. But what does &lt;em&gt;Azure Culture&lt;/em&gt; in a DevOps organization mean? Let&apos;s go back and see how our original question changes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What practises and tools should I adopt which will enable&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;faster change adoption&lt;/strong&gt; in Azure?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;more automation&lt;/strong&gt; in Azure?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;more shared process&lt;/strong&gt; information in Azure?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;more closed feedback&lt;/strong&gt; loops in Azure?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Note - Should we limit ourselves to Azure specific tools. What if we work on Scala, Node, .NET? So perhaps, we need to analyse tooling more comprehensively.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The above is the kind of question that completely derails initiatives if we sit down to exhaustively analyse the options. Sometime you just pick the first option and go - just so that it sets an operational context, if nothing else.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So we will just set one - the .NET stack. &lt;em&gt;Keeping things within the family, you see. You pick scala, or hadoop or node as per your needs. After all, you need multiple iterations of this.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So our task has now decomposed into a search for tooling options on the Azure platform and the .NET software development stack that enables foundational DevOps practises while achieving one or more of our goals - &lt;em&gt;faster change adoption, more automation,more shared process information, more closed feedback loops&lt;/em&gt;. Once we identify tools and technologies, we &lt;em&gt;assess our maturity levels on identified tools and practises&lt;/em&gt; to &lt;em&gt;identify &lt;strong&gt;gaps&lt;/strong&gt; which can be plugged&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The gaps are what we attack in our DevOps strategy 101&lt;/strong&gt;. The number of iterations of this process you go through, and the reviews with all concerned stakeholders should pare the list to items with highest overall priority.&lt;/p&gt;
&lt;p&gt;Now we will do a non technical map-reduce. In &lt;em&gt;map&lt;/em&gt; phase, we list out every possible tool, practise or activity that looks like it might help. In &lt;em&gt;reduce&lt;/em&gt; phase - we prioritize items from list.&lt;/p&gt;
&lt;p&gt;And in map phase our goals are simple.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;tools we can adopt&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;practises we can encourage&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;activities we can do&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, the initial list will probably be large, as both Azure and Microsoft .NET are huge eco-systems, but remember two things,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are just looking for what to start with.&lt;/li&gt;
&lt;li&gt;As of now, we are just identifying as many options as we can that are applicable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But we do need to manage, classify and process this list. Wikipedia, in its DevOps page, suggests the SDLC stages - Code, Test, Build/Package/Relase (&lt;em&gt;merged these as boundaries between them are overlapping, especially in the matter of tooling&lt;/em&gt;), and Monitor.&lt;/p&gt;
&lt;h3&gt;Azure/.NET Stack | SDLC Stage - Code&lt;/h3&gt;
&lt;p&gt;A basic DevOps practise here is &lt;em&gt;Version Control&lt;/em&gt;. Distributed VCS are now standard, so we pick &lt;em&gt;&lt;strong&gt;Git&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Architectural support targeting DevOps enablement for Azure PaaS systems is a much-needed eco-system centric practise. Enter &lt;em&gt;&lt;strong&gt;Microservice architectures&lt;/strong&gt;&lt;/em&gt; which by encouraging small pieces and plug-and-play composition help keeping pieces small and nimble. Smaller pieces offer much greater ability to respond to change, better automation and easier deployment options.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Test-Driven Design (TDD)&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;Domain Driven Design (DDD)&lt;/strong&gt;&lt;/em&gt; are other standard practises that help in better code and decoupled pieces.&lt;/p&gt;
&lt;p&gt;Thus&lt;/p&gt;
&lt;h4&gt;Tools&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Practises&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Test Driven Design (TDD)&lt;/li&gt;
&lt;li&gt;Domain Driven Design (DDD)&lt;/li&gt;
&lt;li&gt;Microservices architectural style&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Azure/.NET Stack | SDLC Stage - Test&lt;/h3&gt;
&lt;p&gt;Test automation has several flavours&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unit Tests&lt;/li&gt;
&lt;li&gt;Integration Tests&lt;/li&gt;
&lt;li&gt;Performance tests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, we can also automate the test generation process through Behavior Driven Design Tools like SpecFlow, and test execution automation is usually done through build pipelines and Continuous Integration (CI)/Continuous Delivery(CD).&lt;/p&gt;
&lt;h4&gt;Tools&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/visualstudio/test/intellitest-manual/introduction&quot;&gt;Pex - Generates boundary conditions tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unit And Integrated Testing Frameworks&lt;/li&gt;
&lt;li&gt;Identify Metrics and tools to report Metrics - NCover, Ndepend, Ncrunch&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Azure Dev/Test Labs&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some possible activities. The one around &lt;em&gt;logging formats&lt;/em&gt; and &lt;em&gt;test reports&lt;/em&gt; is especially interesting -&lt;/p&gt;
&lt;h4&gt;Activities&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;There seems to be no standards in the world around Logging formats and Test reports&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Persona based metrics - &lt;em&gt;who needs what metrics?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Automation ROI graphs - &lt;em&gt;how to demonstrate?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;How to enable specification to final product traceability?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Azure .NET Stack | SDLC Stage - Build/Package/Release&lt;/h3&gt;
&lt;p&gt;In this section, practises are common across software development verticals and horizontals.&lt;/p&gt;
&lt;h4&gt;Practises&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;One Touch Build and Deploys.&lt;/li&gt;
&lt;li&gt;Build Pipelines - Same builds on dev and test machines, Setup builds and Automated local deployment.&lt;/li&gt;
&lt;li&gt;Automated provisioning &lt;em&gt;(Infrastructure configuration and management and Infrastructure as Code tools)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Tools&lt;/h4&gt;
&lt;p&gt;These include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automation tools (in and out-system) - Powershell, MSDeploy,  Puppet, Chef, Docker, Ansible, OctopusDeploy.&lt;/li&gt;
&lt;li&gt;Build tools and Servers (in and out-system) - MSBuild, PSake, DACPAC, TeamCity, Jenkins.&lt;/li&gt;
&lt;li&gt;Eco System specific tools - Azure Resource Management (ARM) Templates and Azure CLI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Azure/.NET Stack | SDLC Stage - Monitor&lt;/h3&gt;
&lt;p&gt;This is probably a big problem area. For now we will gloss over it, but in practise, we would now probably take just monitoring as a problem area for DevOps and resort to the same technique we used throughout this post to break that down into tools, practises and activities.&lt;/p&gt;
&lt;h4&gt;Practises&lt;/h4&gt;
&lt;p&gt;What is needed here is feedback loops and operation trend monitoring through shared dashboards. To large extents, these feedback loops across localized scope (from a DevOps point of view) can be provided by ALM tools like JIRA on the scope and requirements and features perspective, while build and test feedback loops can be provided by CI servers and build-pipelines.&lt;/p&gt;
&lt;p&gt;The difficult part is actually setting up on what to record and monitor. Sifting noise from signal here is a significant and &lt;em&gt;not-always-technical&lt;/em&gt; step. And once past this, we have the technically demanding part in setting up a system which integrates separate local scopes into integrated dashboards. That is hard and definitely not DevOps 101. Maybe later.&lt;/p&gt;
&lt;p&gt;Tools include VSTS - for application life-cycle management, Azure AppInsight for application operation management, standard external tools like NewRelic, and ELK, advanced services like Azure OMS (native) and Google Analytics (external).&lt;/p&gt;
&lt;h4&gt;Activities&lt;/h4&gt;
&lt;p&gt;Following activities are required.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run Maturity Model evaluation on standard tools.&lt;/li&gt;
&lt;li&gt;Establish Personas for Dashboards&lt;/li&gt;
&lt;li&gt;Estrablish Metrics by View/Persona&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At some point, build a &lt;strong&gt;Data Collection and Aggregation Tooling/Implementation&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;&lt;em&gt;Big List of Tools, Practises and Activities&lt;/em&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;What we have been doing so far, is basically running the &lt;em&gt;map&lt;/em&gt; part of a map-reduce analysis. Aggregating all of the tools, practises and activities found above, we get the following big list.&lt;/p&gt;
&lt;h3&gt;Tools / Practises&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Selenium&lt;/li&gt;
&lt;li&gt;Build pipelines and CI&lt;/li&gt;
&lt;li&gt;PowerShell&lt;/li&gt;
&lt;li&gt;Azure CLI&lt;/li&gt;
&lt;li&gt;Azure Dev/Test Labs&lt;/li&gt;
&lt;li&gt;Azure Operations Management Services&lt;/li&gt;
&lt;li&gt;Azure AppInsights.&lt;/li&gt;
&lt;li&gt;Azure Application Resource Templates&lt;/li&gt;
&lt;li&gt;VSTS&lt;/li&gt;
&lt;li&gt;Google Analytics&lt;/li&gt;
&lt;li&gt;NewRelic&lt;/li&gt;
&lt;li&gt;ELK&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Practises&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Microservices&lt;/li&gt;
&lt;li&gt;TDD&lt;/li&gt;
&lt;li&gt;DDD&lt;/li&gt;
&lt;li&gt;BDD&lt;/li&gt;
&lt;li&gt;Automated Unit Tests&lt;/li&gt;
&lt;li&gt;Automated Integration Tests&lt;/li&gt;
&lt;li&gt;One Touch Build and Deploys.&lt;/li&gt;
&lt;li&gt;Build Pipelines - Same builds on dev and test machines, Setup builds and Automated local deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Activities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;There seems to be no standards in the world around Logging formats and Test reports&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Persona based metrics - &lt;em&gt;who needs what metrics?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Automation ROI graphs - &lt;em&gt;how to demonstrate?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;How to enable specification to final product traceability?&lt;/li&gt;
&lt;li&gt;Maturity Model evaluation on identified tools.&lt;/li&gt;
&lt;li&gt;Establish Personas for Dashboards&lt;/li&gt;
&lt;li&gt;Establish Metrics by View/Persona&lt;/li&gt;
&lt;li&gt;Establish Data Collection and Aggregation Tooling/Implementation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This may not be exactly the list you come up with, but if you followed along on the exercise model, you have your own list.&lt;/p&gt;
&lt;p&gt;I am leaving the reduce part of this operation out, as that will probably diverge for everyone. At its basic premise, evaluating your group&apos;s maturity model on each of these items and identifying the highest priority items should get you there. Multiple rounds of reviews from multiple stake-holders is the way forward now, but the no. of items should &apos;&lt;em&gt;reduce&lt;/em&gt;&apos; in reduce phase.&lt;/p&gt;
&lt;p&gt;So there you are, with a set of starting points for a DevOps 101 Strategy. All we have to do now is Go forth, and Iterate.&lt;/p&gt;
&lt;p&gt;Oh, and of course, implement !&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Azure AD and Office 365 OAuth integration through browsers and Postman.</title>
      <link>https://insen.github.io/blog/2017/09/24/Azure-AAD-with-Office-365/</link>
      <pubDate>Sun, 24 Sep 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/09/24/Azure-AAD-with-Office-365/</guid>
      <description>&lt;p&gt;I spent last week answering a question. Is there a way to find available meeting times on a given user&apos;s Office 365 calendar next week? The short answer is &lt;strong&gt;yes&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Longer answer.&lt;/h2&gt;
&lt;p&gt;At work we have MSDN Azure subscriptions. These are linked to our organization&apos;s Azure Active Directory (AAD) and we can sign on to Azure with our Windows credentials. We also Office 365 accounts linked to our Exchange AD and Azure AD. I can seamlessly navigate to Office 365 on the browser after logging into Azure portal through my Windows account (OAuth magics). If this works for you too, then your Single Sign On (SSO) setup is also done.&lt;/p&gt;
&lt;p&gt;The goal here is to understand how Azure OAuth authentication works when calling the outlook APIs through Azure AD. Preferably, I want to achieve this through raw HTTP calls, using just the browsers or Postman REST client, so that the HTTP based protocols and interactions are clear and open to visual inspection. In a nutshell, the steps are as follows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create an Azure AD application / Service principal.&lt;/li&gt;
&lt;li&gt;Redirect user to Exchange Active Directory for Authentication.&lt;/li&gt;
&lt;li&gt;Return Auth Code to user after Azure AD, through OAuth, has authenticated against organization&apos;s Exchange AD.&lt;/li&gt;
&lt;li&gt;Convert OAuth Code into Azure bearer token.&lt;/li&gt;
&lt;li&gt;Call Microsoft Office APIs in SSO mode using token received above to retrieve the available meeting times for a employee given a specified time window.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, many online resources already exist on this, yet getting all the pieces playing together was rough. So, we&apos;ll walk the route I did and hope that the eco-system clears itself up.&lt;/p&gt;
&lt;p&gt;In the process, I will briefly touch on OAuth in Azure, Azure AD, Scopes and Resources in MS Online API, Azure Service Principals aka App registrations, App permissions aka OAuth &lt;em&gt;on-behalf-of consent&lt;/em&gt; flow, Azure bearer tokens in Postman, JSON Web Tokens (JWT) and the Microsoft Graph explorer. Oh! and the Graph and Outlook sandboxes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: In Azure, things change. The information here is of 24th Sept 2017.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For the rest you need an Azure subscription, an Office 365 account, and an Azure AD membership. You can have non work accounts for all three, but its a lot easier if your company admin has done the configuring for you. Blogs are the way to go here if you want to setup trial accounts for each.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;findMeetingTimes API&lt;/h2&gt;
&lt;p&gt;Do such APIs exist? Well, yes, they do. Problem is, more than &lt;strong&gt;&lt;em&gt;one&lt;/em&gt;&lt;/strong&gt; exist.&lt;/p&gt;
&lt;h4&gt;Outlook/Office 365 APIs&lt;/h4&gt;
&lt;p&gt;Documentation link here - &lt;a href=&quot;https://msdn.microsoft.com/en-us/office/office365/api/calendar-rest-operations#find-meeting-times&quot;&gt;https://msdn.microsoft.com/en-us/office/office365/api/calendar-rest-operations#find-meeting-times&lt;/a&gt;,
The API is shown below -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POST https://outlook.office.com/api/{version}/me/findmeetingtimes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and there are &lt;em&gt;three&lt;/em&gt; &lt;em&gt;version&lt;/em&gt;s of it - &lt;em&gt;v1.0, v2.0, beta&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Moreover, to use the API, the docs mention that we also need at least one of the following scopes -  &lt;code&gt;https://outlook.office.com/calendars.read.shared, wl.calendars, wl.contacts_calendars&lt;/code&gt;. Scopes are an important concept in Azure auth governing permissions, and we&apos;ll see more of them in later sections.&lt;/p&gt;
&lt;p&gt;The sandbox to play-around with all of this is here &lt;a href=&quot;https://oauthplay.azurewebsites.net/&quot;&gt;https://oauthplay.azurewebsites.net/&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Microsoft Graph APIs&lt;/h4&gt;
&lt;p&gt;This is the second, latest and greatest option from Microsoft. Again, documentation is here - &lt;a href=&quot;https://developer.microsoft.com/en-us/graph/docs/concepts/findmeetingtimes_example&quot;&gt;https://developer.microsoft.com/en-us/graph/docs/concepts/findmeetingtimes_example&lt;/a&gt;, the API looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POST https://graph.microsoft.com/v1.0/me/findMeetingTimes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the sandbox (called MS Graph Explorer), is here - &lt;a href=&quot;https://developer.microsoft.com/en-us/graph/graph-explorer&quot;&gt;https://developer.microsoft.com/en-us/graph/graph-explorer&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Outlook vs Graph.&lt;/h4&gt;
&lt;p&gt;So why two? When to use what?&lt;/p&gt;
&lt;p&gt;Well previously MS had multiple APIs available - Office, Azure AD, etc. MS Graph is intended to unify all of these endpoints into a single REST-ful gateway for all of Microsoft&apos;s underlying platform APIs viz. Azure AD, Excel, Outlook, OneDrive, OneNote, SharePoint etc. It is not fully at par with existing APIs, but Microsoft&apos;s recommendation is to prefer MS Graph unless you need a feature which it does not have.&lt;/p&gt;
&lt;p&gt;For current and future work, prefer MS Graph over other means.&lt;/p&gt;
&lt;h4&gt;API Request body.&lt;/h4&gt;
&lt;p&gt;In either case the POST body is the same, so that&apos;s a relief. A detailed request is given towards the end of this post.&lt;/p&gt;
&lt;p&gt;So endpoints - Check.&lt;/p&gt;
&lt;p&gt;Request Body and Response - Check.&lt;/p&gt;
&lt;p&gt;Well, on to Azure AD and OAuth SSO, then.&lt;/p&gt;
&lt;h2&gt;Azure Active Directory (AAD) and OAuth.&lt;/h2&gt;
&lt;p&gt;Azure OAuth based authentication is a big and complex topic and cannot be covered adequately in a single blog post. Conceptually, the Azure OAuth flow is like &lt;img src=&quot;/blog/img/posts/azureoauth.png&quot; alt=&quot;this&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The diagram source and more documentation can be found at &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-v2-protocols-oauth-code#main&quot;&gt;https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-v2-protocols-oauth-code#main&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our case, as mentioned before, the full authentication process needs the following -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create an Azure AD application / Service principal.&lt;/li&gt;
&lt;li&gt;Redirect user to Exchange Active Directory for Authentication.&lt;/li&gt;
&lt;li&gt;Return Auth Code to user after Azure AD, through OAuth, has authenticated against organization&apos;s Exchange AD.&lt;/li&gt;
&lt;li&gt;Convert OAuth Code into Azure bearer token.&lt;/li&gt;
&lt;li&gt;Call Microsoft Office APIs in SSO mode using token received above to retrieve the available meeting times for a employee given a specified time window.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But lets look at the details, where the devils lurk.&lt;/p&gt;
&lt;h4&gt;Theory - App Registration / Generate Service Principal&lt;/h4&gt;
&lt;p&gt;First, someone needs to get authenticated. This, in Azure terms, is a Service Principal. This is achieved by registering an application with Azure AD, which gives us three important keys, the &lt;em&gt;tenantId, clientId&lt;/em&gt; and the &lt;em&gt;clientSecret&lt;/em&gt; after registration.
&lt;em&gt;For details about how to register an app, see  &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-integrating-applications&quot;&gt;https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-integrating-applications&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After registration, we save the following key-value pairs for subsequent use.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;tenantId&lt;/em&gt;: guid looking code of the AD instance in your Azure subscription.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;clientId&lt;/em&gt;: guid looking code of the application being registered.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;clientSecret&lt;/em&gt;: guid looking key corresponding to a code we create in the application properties on Azure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Note : The clientSecret will be shown only once when creating a app property. If you don&apos;t note it down, you need to delete and recreate a key. It canot be recovered.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Theory - Azure AD and OAuth.&lt;/h4&gt;
&lt;p&gt;The OAuth dance is a two-step process here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get a authentication code from the underlying authentication provider (OpenId, Active Directory).&lt;/li&gt;
&lt;li&gt;Convert that code into a &lt;a href=&quot;https://jwt.io&quot;&gt;JSON Web Token&lt;/a&gt;. For subsequent calls, this token needs to be used as the Authorization header.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Theory - Authorize API&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following are the current logon/authorization endpoints for both v1.0 and v2.0.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://login.microsoftonline.com/{tenantId}/oauth2/authorize
https://login.microsoftonline.com/{tenantId/oauth2/v2.0/authorize
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and a sample logon request with query parameters is as follows -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET https://login.microsoftonline.com/common/oauth2/v2.0/authorize?client_id=CLIENT_ID&amp;amp;redirect_uri=http%3A%2F%2Flocalhost%2Fmyapp%2F&amp;amp;response_type=code&amp;amp;scope=openid+https://office.outlook.com/Calendars.Read.Shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The query parameter definitions are given below -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-   client_id: the clientId generated before during registering the app. This lets Azure know which app/service principal is requesting the logon.
-   redirect_uri: the location that Azure will redirect to once the user has granted consent to the app. This value must correspond to the value of Redirect URI used when *registering the app*.
-   response_type: the type of response the app is expecting. For the Authorization Grant Flow, this should always be the string `code`.
-   scope: a space-delimited list of access scopes that the app requires. For access to all outlook shared readable calendars in our Active Directory We have specified the following 
    - scope=openid+https://office.outlook.com/Calendars.Read.Shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Theory - Key things to remember&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Previous versions had the domain &lt;code&gt;login.windows.net&lt;/code&gt;. Some blogs use this. This is now not exposed, even though, based on inspection of HTTP headers, can be seen to be active in the background.&lt;/li&gt;
&lt;li&gt;Also note we have two versions of the current endpoint as well - v1.0 and v2.0, and v2.0 has some differences from v1.0, viz., &lt;em&gt;v2.0 endpoint does not understand the query parameter &apos;resource&apos; and throws an error&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;For multi-tenant apps, replace &lt;em&gt;tenantId&lt;/em&gt; with &lt;em&gt;&apos;common&apos;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The keys given to us from Azure app registration are &lt;strong&gt;&lt;em&gt;clientId, tenantId, clientSecret&lt;/em&gt;&lt;/strong&gt;. However, in the authorize call, the API expects the query parameters as &lt;em&gt;underscore_separated&lt;/em&gt;, i.e., as &lt;strong&gt;&lt;em&gt;client_id, tenant_id, and client_secret&lt;/em&gt;&lt;/strong&gt;. I got stuck here for a while too.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Theory - OAuth response&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The authorization takes the required query string parameters, in our case, returns an authentication code. This is one of the possible variations on the request.&lt;/p&gt;
&lt;p&gt;Using OAuth, authentication is redirected to your organizations Windows Active Directory SSO page, so that you can log in with your Windows credentials.&lt;/p&gt;
&lt;p&gt;It then returns the code to the &lt;em&gt;response_uri&lt;/em&gt; specified. The &lt;em&gt;code&lt;/em&gt; looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;QABAAIAAAABlDrqfEFlSaui6xnRjX5Ef_{removed_lots_of_crazy_characters_here}_OSXEQcdFf2RLPAXbz30RgbyAA
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Theory - Scopes in brief.&lt;/h4&gt;
&lt;p&gt;Scopes correspond to permissions which the account making the Rest API call will need to successfully interact with Azure resources. In our case, it the application corresponding to the &lt;em&gt;clientId&lt;/em&gt; which needs the permissions, as we are logging on to our tenant as this client, i.e. we will use this &lt;em&gt;clientId&lt;/em&gt; during the logon/authorize call.&lt;/p&gt;
&lt;p&gt;Roughly speaking, we can think of an Azure Resource equals an domain viz. &lt;code&gt;office.outlook.com, graph.microsoft.com,&lt;/code&gt; etc. Note that Azure resources can be a lot more granular though.&lt;/p&gt;
&lt;p&gt;The application we registered can request these set of permissions after the logon page is past,
OR we can pre-allow these permissions to the application while registering. This option is the &lt;em&gt;OAuth On-Behalf-Of Consent Flow&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A fuller discussion on scopes can be found at &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-v2-scopes&quot;&gt;https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-v2-scopes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A key thing to remember is the difference between how scopes are provided for Outlook REST APIs at &lt;code&gt;outlook.office.com&lt;/code&gt; vs. MS Graph at &lt;code&gt;graph.microsoft.com&lt;/code&gt;. For Outlook - &lt;em&gt;resource&lt;/em&gt; permissions (permissions for a particular URL) need to be specified in the query parameter in the format &lt;code&gt;https://url/permissions&lt;/code&gt;. See example below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openid+offline access+https://outlook.office.com/Calendars.Read.Shared+https://outlook.office.com/Contacts.Read
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For Graph API, the URL can be omitted, so the query param for MS Graph will be&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openid+offline access+Calendars.Read.Shared+Contacts.Read
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not need &lt;code&gt;https://graph.microsoft.com&lt;/code&gt; in the URL&apos;s &lt;em&gt;scope&lt;/em&gt; parameter.&lt;/p&gt;
&lt;h3&gt;Theory - Convert Auth Code To Bearer Token&lt;/h3&gt;
&lt;p&gt;The second part of authentication is in converting the auth code into a JSON Web Token (see &lt;code&gt;https://jwt.io&lt;/code&gt;). The base tokenization URL is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POST https://login.windows.net/common/oauth2/token
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The body is of type www-form-encoded and has the following key-value pairs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;grant_type=authorization_code&amp;amp;code={code from the authorize request}&amp;amp;redirect_uri={reply url for your application}&amp;amp;client_id={your application&apos;s client id in AAD}&amp;amp;client_secret={your application&apos;s client secret}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The HTTP response is a JSON document of form -&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{
    &amp;quot;token_type&amp;quot;: &amp;quot;Bearer&amp;quot;,
    &amp;quot;scope&amp;quot;: &amp;quot;Calendars.Read Calendars.Read.All Calendars.Read.Shared offline_access&amp;quot;
    &amp;quot;expires_in&amp;quot;: &amp;quot;3599&amp;quot;,
    &amp;quot;ext_expires_in&amp;quot;: &amp;quot;262800&amp;quot;,
    &amp;quot;expires_on&amp;quot;: &amp;quot;1506082448&amp;quot;,
    &amp;quot;not_before&amp;quot;: &amp;quot;1506078548&amp;quot;,
    &amp;quot;resource&amp;quot;: &amp;quot;https://outlook.office.com/&amp;quot;,
    &amp;quot;access_token&amp;quot;: &amp;quot;{removed-use your own tokens}&amp;quot;,
    &amp;quot;refresh_token&amp;quot;: &amp;quot;{removed-use your own tokens}&amp;quot;,
    &amp;quot;id_token&amp;quot;: &amp;quot;{removed-use your own tokens}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A access token is as per the JWT specification. Encoded, it looks like this - &lt;code&gt;eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1-{removed_lots_of_chars_here}-CTN2M3GXpo53GaXko0FgwPFjPA&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;However, this contains a lot of information. When decoded (via &lt;a href=&quot;http://jwt.io&quot;&gt;JWT.io&lt;/a&gt;), we can see the information it contains -&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{
  &amp;quot;aud&amp;quot;: &amp;quot;https://outlook.office.com/&amp;quot;,
  &amp;quot;iss&amp;quot;: &amp;quot;https://sts.windows.net/{tenantId}/&amp;quot;,
  &amp;quot;iat&amp;quot;: 1506070849,
  &amp;quot;nbf&amp;quot;: 1506070849,
  &amp;quot;exp&amp;quot;: 1506074749,
  &amp;quot;acr&amp;quot;: &amp;quot;1&amp;quot;,
  &amp;quot;aio&amp;quot;: &amp;quot;Y2VgYHjuJj1PSJv1qLb8h9Y9s3wvRa5NSegLEFNqsHqx9d6LHgcA&amp;quot;,
  &amp;quot;amr&amp;quot;: [
    &amp;quot;pwd&amp;quot;
  ],
  &amp;quot;appid&amp;quot;: &amp;quot;registered_app_id&amp;quot;,
  &amp;quot;appidacr&amp;quot;: &amp;quot;1&amp;quot;,
  &amp;quot;e_exp&amp;quot;: 262800,
  &amp;quot;enfpolids&amp;quot;: [],
  &amp;quot;family_name&amp;quot;: &amp;quot;my_family_name&amp;quot;,
  &amp;quot;given_name&amp;quot;: &amp;quot;my_first_name&amp;quot;,
  &amp;quot;ipaddr&amp;quot;: &amp;quot;219.91.160.98&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;my_full_name&amp;quot;,
  &amp;quot;oid&amp;quot;: &amp;quot;oid&amp;quot;,
  &amp;quot;onprem_sid&amp;quot;: &amp;quot;S-1-5-21-1708537768-789336058-725345543-1974208&amp;quot;,
  &amp;quot;puid&amp;quot;: &amp;quot;1003BFFD870F4BB4&amp;quot;,
  &amp;quot;scp&amp;quot;: &amp;quot;Calendars.Read Calendars.Read.All Calendars.Read.Shared Contacts.Read Contacts.Read.Shared offline_access&amp;quot;,
  &amp;quot;sub&amp;quot;: &amp;quot;5M4_YhrPEtd558TCLIWVHGxhgKyxqsctKvtjYpnEx1o&amp;quot;,
  &amp;quot;tid&amp;quot;: &amp;quot;tenantId&amp;quot;,
  &amp;quot;unique_name&amp;quot;: &amp;quot;my logon mail id here&amp;quot;,
  &amp;quot;upn&amp;quot;: &amp;quot;my logon mail id here&amp;quot;,
  &amp;quot;ver&amp;quot;: &amp;quot;1.0&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Theory - MS Office API, REST call.&lt;/h3&gt;
&lt;p&gt;Most of the nitty-gritties should have been worked out of the way now. We just need to call the Outlook REST API as follows -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POST https://outlook.office.com/api/{version}/me/findmeetingtimes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the following HTTP headers to the request&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Content-Type : application/json&lt;/li&gt;
&lt;li&gt;Authorization : bearer &lt;em&gt;{token_returned_by_token_endpoint}&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; : Note the string &lt;em&gt;bearer&lt;/em&gt;, followed by a space, in front of the token. I struggled for a long time at this step before figuring this out.&lt;/p&gt;
&lt;p&gt;And voila. everything works. or does it? Since practise beats theory everytime, let&apos;s get down to browsers and POSTMAN.&lt;/p&gt;
&lt;h2&gt;Practise - Putting it all together.&lt;/h2&gt;
&lt;p&gt;Okay, brass-tacking time now.&lt;/p&gt;
&lt;h4&gt;Practise - Add Application into AD.  Save the &lt;em&gt;tenantId, clientId and clientSecret&lt;/em&gt;.&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Added a new application called &apos;Postman&apos; in the linked Azure Active Directory through Azure portal.&lt;/li&gt;
&lt;li&gt;Specify &apos;Read User and Shared Calendars&apos; in the permissions panel, and explicitly granted those permissions. Through this, we are implementing &lt;em&gt;OAuth On-Behalf-Of-Consent flow&lt;/em&gt;. Otherwise, we would have an intermediate screen after entering our Windows credentials where we would have to accept the use of these permissions.&lt;/li&gt;
&lt;li&gt;Add two response_uri, one for Postman REST client to use the response auth code, and one to see the raw http auth request and response in a browser window myself. For Postman I added &lt;code&gt;https://www.getpostman.com/oauth2/callback&lt;/code&gt;, as per Postman REST client requirements, and a second one, &lt;code&gt;http://localhost/myapp/&lt;/code&gt; for my browser based calls.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once done, we keep the following key-values (&lt;em&gt;not exact, of course&lt;/em&gt;) handy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;tenantId : fea858f0-512d-4649-8228-d78fd9ef3c7f
clientId : 794b53a6-e176-4185-92fe-617dd8512db5
clientSecret: MyQt+7mKw/Vz7p6XjHRfBOe68ffWvjmjVhJP69K1dec!=
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Practise - Invoke Azure Authorize API.&lt;/h4&gt;
&lt;p&gt;The full &lt;code&gt;GET&lt;/code&gt; request for my use case is as follows. Paste this into a browser window which is not already signed into azure.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://login.microsoftonline.com/common/oauth2/authorize?client_id=794b53a6-e176-4185-92fe-617dd8512db5&amp;amp;redirect_uri=http%3A%2F%2Flocalhost%2Fmyapp%2F&amp;amp;response_type=code&amp;amp;scope=https://outlook.office.com/calendars.read.shared
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Important points to remember&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note I have used the endpoint &apos;common&apos;.&lt;/li&gt;
&lt;li&gt;Note that I have used v1.0 endpoint. v2.0 did not work for me. The Authorize call succeeds, but the token call fails with a version mismatch error.&lt;/li&gt;
&lt;li&gt;Note the callback used here &apos;http://localhost/myapp/&apos;. This was one of the endpoints registered with AD as the call back URI for the &apos;Postman&apos; application/service principal.&lt;/li&gt;
&lt;li&gt;Ensure that all URLs mentioned are escaped properly. in the above the &lt;em&gt;request_uri&lt;/em&gt; is escaped right, but the scope one isn&apos;t. It still works, but try not to do this.&lt;/li&gt;
&lt;li&gt;Ensure that all URLs end with a trailing slash. This one causes a nasty error with an incomprehensible error message.&lt;/li&gt;
&lt;li&gt;Add the scopes correctly and ensure that the corresponding permissions are granted in Azure portal to the Application.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When the call returns, it shows me a browser page with HTTP 404. That is expected, as the actual URL does not exist. The URL that can be seen in the browser address bar, is like this -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://localhost/myapp/?code=QABAAIAAAABlDrqfEFlSaui6-{removed_lots_of_chars_here}-e3e45d8d4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this is the callback URL specified during application registration. The auth code is appended to the querystring. We will need this code subsequently.&lt;/p&gt;
&lt;h4&gt;Practise - Invoke Azure token API&lt;/h4&gt;
&lt;p&gt;In POSTMAN, Create a new &lt;code&gt;POST&lt;/code&gt; request to the following endpoint&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://login.microsoftonline.com/common/oauth2/token
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a request body as type &lt;code&gt;x-www-form-urlencoded&lt;/code&gt;. In the POSTMAN bulk-edit mode, add the following JSON parameters to it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;client_id: 794b53a6-e176-4185-92fe-617dd8512db5
scope: https://outlook.office.com/calendars.read.shared/
resource: https://outlook.office.com/
code: QABAAIAAAABlDrq-{removed_lots_of_chars_here}-UassQOSXEQcdFf2RLPAXbz30RgbyAA
redirect_uri: http://localhost/myapp/
grant_type: authorization_code
client_secret: MyQt+7mKw/Vz7p6XjHRfBOe68ffWvjmjVhJP69K1dec!=
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fire the request. We should get the following response.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;token_type&amp;quot;: &amp;quot;Bearer&amp;quot;,
    &amp;quot;scope&amp;quot;: &amp;quot;Calendars.Read Calendars.Read.All Calendars.Read.Shared Contacts.Read Contacts.Read.Shared offline_access&amp;quot;,
    &amp;quot;expires_in&amp;quot;: &amp;quot;3599&amp;quot;,
    &amp;quot;ext_expires_in&amp;quot;: &amp;quot;262800&amp;quot;,
    &amp;quot;expires_on&amp;quot;: &amp;quot;1506082448&amp;quot;,
    &amp;quot;not_before&amp;quot;: &amp;quot;1506078548&amp;quot;,
    &amp;quot;resource&amp;quot;: &amp;quot;https://outlook.office.com/&amp;quot;,        
    &amp;quot;access_token&amp;quot;: &amp;quot;myJ0eXAiOiJKV1Qi-{removed_lots_of_chars_here}-ukX3nBlRfzh6Sg&amp;quot;,        
    &amp;quot;refresh_token&amp;quot;: &amp;quot;QABAAAAAAABlDrq-{removed_lots_of_chars_here}-uoBFATkE6ZJHyAA&amp;quot;,       
    &amp;quot;id_token&amp;quot;: &amp;quot;yJ0eXAiOiJKV1QiL-{removed_lots_of_chars_here}-SIsInZlciI6IjEuMCJ9.&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Practise - Invoke Outlook REST API.&lt;/h4&gt;
&lt;p&gt;Again in POSTMAN, create another &lt;code&gt;POST&lt;/code&gt; request for the following URI.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://outlook.office.com/api/v2.0/me/findmeetingtimes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the HTTP headers&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Content-Type: application/json,
Authorization: bearer myJ0eXAiOiJKV1-{removed_lots_of_chars_here}-PNukX3nBlRfzh6Sg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; the space between bearer and the rest of the &lt;em&gt;access_token&lt;/em&gt; returned by the previous token call.&lt;/p&gt;
&lt;p&gt;Add the following as raw request JSON -&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{ 
&amp;quot;Attendees&amp;quot;: [ 
    { 
    &amp;quot;Type&amp;quot;: &amp;quot;Required&amp;quot;,  
    &amp;quot;EmailAddress&amp;quot;: { 
        &amp;quot;Name&amp;quot;: &amp;quot;my_name&amp;quot;,
        &amp;quot;Address&amp;quot;: &amp;quot;my_mail_id&amp;quot; 
    } 
    },
{ 
    &amp;quot;Type&amp;quot;: &amp;quot;Optional&amp;quot;,  
    &amp;quot;EmailAddress&amp;quot;: { 
        &amp;quot;Name&amp;quot;: &amp;quot;other_name&amp;quot;,
        &amp;quot;Address&amp;quot;: &amp;quot;other_mail_id&amp;quot; 
    } 
    }  
],  
&amp;quot;TimeConstraint&amp;quot;: { 
    &amp;quot;ActivityDomain&amp;quot;:&amp;quot;Work&amp;quot;,
    &amp;quot;Timeslots&amp;quot;: [ 
    { 
        &amp;quot;Start&amp;quot;: { 
        &amp;quot;DateTime&amp;quot;: &amp;quot;2017-09-22T07:00:00&amp;quot;,  
        &amp;quot;TimeZone&amp;quot;: &amp;quot;Pacific Standard Time&amp;quot; 
        },  
        &amp;quot;End&amp;quot;: { 
        &amp;quot;DateTime&amp;quot;: &amp;quot;2017-09-23T17:00:00&amp;quot;,  
        &amp;quot;TimeZone&amp;quot;: &amp;quot;Pacific Standard Time&amp;quot; 
        } 
    } 
    ] 
},  
&amp;quot;MeetingDuration&amp;quot;: &amp;quot;PT1H&amp;quot; 
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fire the request.  The outlook API should now return a valid response from Office 365.&lt;/p&gt;
&lt;p&gt;If you want to try out the equivalent graph API, you need to do a fresh authentication. This time use the scopes as required for MS Graph &lt;em&gt;(no url, just the scope value)&lt;/em&gt;. Then use this Auth Code to get a fresh bearer token. Using this token for a call to the MS Graph API should work. It did work for me without any hassles.&lt;/p&gt;
&lt;p&gt;And that&apos;s all of it. We can go home now.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Blogging with Github Pages and Pretzel.</title>
      <link>https://insen.github.io/blog/2017/08/29/Blogging-Github-Pretzel/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/08/29/Blogging-Github-Pretzel/</guid>
      <description>&lt;p&gt;As I mentioned in my &lt;a href=&quot;http://insen.github.io/blog/2017/08/29/helloblog/&quot;&gt;&apos;hello-blog&apos;&lt;/a&gt; post, my search for blogging tools ended at Github Pages and Pretzel. In this post, I will outline the process of setting up a blog using Github pages, Pretzel, Hyde and Markdown, using what I learnt while setting up this one.&lt;/p&gt;
&lt;h3&gt;The Goal.&lt;/h3&gt;
&lt;p&gt;The goal here is a system which has the following publish-a-blog sequence -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write the blog in as close to english as possible. We use Markdown (filename.md) for close-to-english text files.&lt;/li&gt;
&lt;li&gt;Add the file into a specific directory on local machine.&lt;/li&gt;
&lt;li&gt;Review add/edited blog-site and added/edited blog-post without internet connectivity. Rinse and repeat as needed.&lt;/li&gt;
&lt;li&gt;Check in all updates (aka backup). Enter Github - On successful check-in, online blog is automatically updated with new content. (Only this step needs internet connectivity).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, its very developer friendly. This process emulates the &lt;strong&gt;&lt;em&gt;code, build, test, deploy&lt;/em&gt;&lt;/strong&gt; cycle of software development.&lt;/p&gt;
&lt;h3&gt;The Pieces.&lt;/h3&gt;
&lt;p&gt;The infrastructure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;Github project pages repository&lt;/strong&gt; which will have
&lt;ul&gt;
&lt;li&gt;A template project for a web-site, which I chose to be &lt;strong&gt;Hyde&lt;/strong&gt;. Why?
&lt;ol&gt;
&lt;li&gt;I liked it.&lt;/li&gt;
&lt;li&gt;It is a Jekyll theme. Github pages default is Jekyll. I avoided using Jekyll because of reasons cited later, But staying close to defaults seemed a good idea.	&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;A directory within the repository named &apos;docs&apos; (as per Github rules) holding the pre-assembled auto-generated and checked-in web-site. Github pages automatically serves this web-site from a public Url.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pretzel&lt;/strong&gt; to build/rebuild the web-site template, new/edited posts and generate/preview the updated web-site while offline. Some batch files for the builds with Pretzel commands, so that the process is seamlessly repeatable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Git&lt;/strong&gt;, to push the updates to the template, the posts and to the generated web-site (docs folder) into the repo and the publishable version into the docs folder. &lt;em&gt;(Note that we didn&apos;t say anything about copying anything over to docs folder - Pretzel build scripts take care of that.)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Oh, and I almost forgot. &lt;strong&gt;Visual Studio Code&lt;/strong&gt; is the text editor. Pick whichever you like.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Note - If you don&apos;t have a github account, this is a good time to create one. Also, if you don&apos;t know Git, much of this blog won&apos;t make sense to you. if you really, really want to blog, this is a good alternative option &lt;a href=&quot;http://tumblr.com&quot;&gt;tumblr&lt;/a&gt;)&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Github and &lt;a href=&quot;https://help.github.com/articles/user-organization-and-project-pages/&quot;&gt;Github Pages&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Github Pages are a feature inbuilt into Github which enables you to host static web-sites with minimum effort. There are a few different flavors.&lt;/p&gt;
&lt;h4&gt;User Pages&lt;/h4&gt;
&lt;p&gt;If your github account is &lt;strong&gt;acc&lt;/strong&gt;, Create a new repository named as &lt;strong&gt;acc&lt;/strong&gt; (same as the account name). A public web-site with no content is available at http://&lt;strong&gt;acc&lt;/strong&gt;.github.io.&lt;/p&gt;
&lt;p&gt;Add a index.html with only the text &apos;Hello World&apos; into the &apos;master&apos; branch.  You should see &apos;Hello World&apos; in your browser from http://&lt;strong&gt;acc&lt;/strong&gt;.github.io/.&lt;/p&gt;
&lt;h4&gt;Organization Pages&lt;/h4&gt;
&lt;p&gt;Similar to user pages except the repo is at the organization level, and has the same name as the
organization. Gets published(approximately) at http://github.com/&lt;strong&gt;organizationname&lt;/strong&gt;. *Check with official Github documentation as I didn&apos;t investigate this much.&lt;/p&gt;
&lt;h4&gt;Project Pages&lt;/h4&gt;
&lt;p&gt;If your github account is &lt;strong&gt;acc&lt;/strong&gt;, viz, your Github account is at https://&lt;strong&gt;acc&lt;/strong&gt;.github.com, and you add a new repository named as &lt;strong&gt;repo&lt;/strong&gt;, the Project pages site with no content is available at https://&lt;strong&gt;acc&lt;/strong&gt;.github.io/&lt;strong&gt;repo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Add a folder called &apos;docs&apos; in repo in the master branch.&lt;/p&gt;
&lt;p&gt;Add a &lt;strong&gt;index.html&lt;/strong&gt; with just the text &apos;Hello World&apos; into the &apos;docs&apos; folder. This automatically gets published as http://&lt;strong&gt;acc&lt;/strong&gt;.github.io/&lt;strong&gt;repo&lt;/strong&gt;. You should see &apos;Hello World&apos; in your browser from this site.&lt;/p&gt;
&lt;p&gt;These sites are available at a per-repository level. They can also be configured so that the web-site is geing served either from master branch, from a top level folder in the master branch named as &lt;strong&gt;docs&lt;/strong&gt; or a branch specifically named as &lt;strong&gt;gh-pages&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I am using the &lt;strong&gt;docs&lt;/strong&gt; option above.&lt;/p&gt;
&lt;h4&gt;Other Things...&lt;/h4&gt;
&lt;p&gt;Note that it is perfectly possible to have user-pages, organization-pages and project-pages in the same account. Given that the user account is &lt;strong&gt;acc&lt;/strong&gt;, the organization name is &lt;strong&gt;organizationname&lt;/strong&gt;, and the project repo name is &lt;strong&gt;repo&lt;/strong&gt;, the following public web-sites will be available.
-	http://&lt;strong&gt;acc&lt;/strong&gt;.github.io/  as the User Page Site.
-	http://github.com/&lt;strong&gt;organizationname&lt;/strong&gt; as an Organization Page Site
-	http://&lt;strong&gt;acc&lt;/strong&gt;.github.io/&lt;strong&gt;repo&lt;/strong&gt;  as a Project Page Site&lt;/p&gt;
&lt;p&gt;The site can be a Jekyll based, &lt;strong&gt;OR&lt;/strong&gt; a plain vanilla html website but with with an index.html at the top level of the site. I presume how this works is this - Github passes a http call to the site url through a web-server which can process &lt;a href=&quot;http://liquid.org&quot;&gt;Liquid&lt;/a&gt; - a html templating engine. This engine passes on plain vanilla html as is, so html spec based content just flows through that web server. We are using this approach as we are pre-building the web-site as a collection of static html pages.&lt;/p&gt;
&lt;p&gt;If using Jekyll, you need to create a Jekyll based website in the repo and branch as per your choice from the page types - user, organization or project. Github&apos;s Jekyll based build process will probably do the Jekyll builds on your behalf before publishing online. More details about Github&apos;s inbuilt support for Jekyll publishing can be found at &lt;a href=&quot;https://help.github.com/articles/user-organization-and-project-pages/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Hello &lt;a href=&quot;https://Github.com/Code52/pretzel&quot;&gt;Pretzel&lt;/a&gt; &lt;em&gt;(and Static-site generators)&lt;/em&gt;!&lt;/h3&gt;
&lt;p&gt;Pretzel is a open source, static-site generator in .NET.&lt;/p&gt;
&lt;p&gt;Maybe the first question to address is how many of these there are? &lt;a href=&quot;https://www.staticgen.com/&quot;&gt;Take a look&lt;/a&gt;. When it comes to frameworks, We are living in a world of plenty these days.&lt;/p&gt;
&lt;p&gt;What do they do? Well, basically a static site generator, especially the ones targeting blogging, take a bunch of layout templates (e.g. site-header.html, sidebar.html, footer.html), your CSS stylesheets, your Javascript, and your posts (usually in markdown, but based on support you could probably use any syntax - markdown, yaml, plain html, razor) - and processes them to a colection of plain vanilla HTML/CSS/Javascript pages - one for each post. Each page generated is a complete and self-contained html page, combining all the common layout templates, the CSS, the  Javascript and the Markdown content, with links between pages and navigation.&lt;/p&gt;
&lt;p&gt;I picked Pretzel because it is written in C#. I know C# well so I can read the code, debug or enhance Pretzel if I ever so require. (Other possible options included &lt;a href=&quot;http://wyam.io&quot;&gt;Wyam&lt;/a&gt;, and &lt;a href=&quot;https://github.com&quot;&gt;Sandra.Snow&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Now Pretzel tries to keep as close as possible to Jekyll. Considering I am looking for a offline Jekyll replacement, Pretzel seemed appropriate.&lt;/p&gt;
&lt;p&gt;And with Pretzel I can build/rebuild/run the entire site on my local machine with minimal fiddling (Which Jekyll setup, debug, or enhancement will all force me to do, and all in Ruby). As a .NET developer, my primary machine is usually all set to do just about anything with the CLR. Once testing is done, all I need to do is copy the local machine site folder into the folder from which Github serves project pages (the &lt;strong&gt;&apos;docs&apos;&lt;/strong&gt; folder under Github repository root). With build scripts, I can automate this step. When I check-in to Github, after allowing some time for Github to build the site and CDN propagation to happen, the latest content comes up on the public site.&lt;/p&gt;
&lt;p&gt;For further details, check out the &lt;a href=&quot;https://Github.com/Code52/pretzel/wiki&quot;&gt;Pretzel wiki&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;h4&gt;Setting up the repository in Github portal.&lt;/h4&gt;
&lt;p&gt;Do the following to setup your Github blogging. This method sets up a Project Site as the blog site.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a Github account, viz. &lt;strong&gt;acc&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Create a repository in github, viz. &lt;strong&gt;repo&lt;/strong&gt;. I created https://github.com/insen/blog.&lt;/li&gt;
&lt;li&gt;Git clone into local machine. This is where you will be adding your site.
&lt;ul&gt;
&lt;li&gt;Add a &lt;strong&gt;docs&lt;/strong&gt; folder. &lt;em&gt;(Name needs to be exact)&lt;/em&gt;.
&lt;ul&gt;
&lt;li&gt;Add an index.html with the text &apos;Hello world&apos;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Push to Github.&lt;/li&gt;
&lt;li&gt;Goto Github repository settings, Gh-Pages section - Select &apos;master branch/docs folder&apos;.&lt;/li&gt;
&lt;li&gt;Check &lt;a href=&quot;#&quot;&gt;https://&lt;strong&gt;acc&lt;/strong&gt;.github.io/repo/&lt;/a&gt; - the contents of index.html should be visible in your browser.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And that&apos;s it. The blog post you are currently reading at &lt;a href=&quot;https://insen.github.io/blog/&quot;&gt;https://insen.github.io/blog/&lt;/a&gt; has been built in the same fashion, though we are still missing a few steps yet - We need to setup pretzel and then make our site look decent. But reversing th order makes things easier, So onto Hyde first, then Pretzel.&lt;/p&gt;
&lt;h4&gt;Importing Hyde to local machine.&lt;/h4&gt;
&lt;p&gt;Hyde is a pre-built theme for Jekyll. Since Pretzel closely follows Jekyll paradigms, It should work without issues, or so I thought. I was mostly correct. All we have to do here is git clone the &lt;a href=&quot;https://github.com/poole/hyde&quot;&gt;Hyde repository at Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reason we need this is that the basic site created by Pretzel is quite horrible, both aesthetically and structurally. I started looking for something that works from an aesthetic point of view, but as close to Jekyll as possible, which led me to &lt;a href=&quot;https://github.com/poole/hyde&quot;&gt;Hyde&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Setting up Pretzel on local machine and basic usage.&lt;/h4&gt;
&lt;p&gt;Setting up Pretzel is quite straight forward. Get Pretzel from &lt;a href=&quot;https://github.com/Code52/pretzel&quot;&gt;here&lt;/a&gt;, &lt;strong&gt;OR&lt;/strong&gt; use Chocolatey - a package manager for windows. To use Chocolatey, install Chocolatey from &lt;a href=&quot;https://chocolatey.org/&quot;&gt;here&lt;/a&gt;, and on an elevated command prompt or powershell console, run the command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;	choco install pretzel
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;To create a basic site for you. The command details are &lt;a href=&quot;https://Github.com/Code52/pretzel/wiki&quot;&gt;here&lt;/a&gt;. You can
lookup the various possible options.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;	pretzel create [options]
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Bake the site - means generate the output website and put it into a default folder. Usually named &lt;strong&gt;_site&lt;/strong&gt; and located at project root level. The command details are &lt;a href=&quot;https://Github.com/Code52/pretzel/wiki&quot;&gt;here&lt;/a&gt;. I will use the following version.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;	pretzel bake 
		--source=&amp;quot;c://srcpath&amp;quot; 
		--destination=&amp;quot;d://dpath&amp;quot; 
		--cleantarget
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Test the site&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&quot;language-batch&quot;&gt;	pretzel taste 
		--source=&amp;quot;c://dpath&amp;quot; 
		--port=8001
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Important note&lt;/em&gt; - the source directory in &lt;em&gt;&apos;taste&apos;&lt;/em&gt; command is the destination directory in &lt;em&gt;&apos;bake&apos;&lt;/em&gt; command. This is what we are using. For other options, check Pretzel wiki.&lt;/p&gt;
&lt;p&gt;I will be creating batch scripts for the &lt;em&gt;(bake)&lt;/em&gt; and &lt;em&gt;(taste)&lt;/em&gt; snippets which will be useful.&lt;/p&gt;
&lt;h3&gt;Getting jiggy with it.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Delete the contents of your repository (except docs folder), and then copy the entire contents of the hyde repo into your blog repo.&lt;/li&gt;
&lt;li&gt;Run the Pretzel bake command from a prompt at the repository root folder with no command-line args. Pretzel should create a folder called &lt;strong&gt;_site&lt;/strong&gt; under repository folder. It does NOT. It does this instead.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&quot;language-c#&quot;&gt;	Unhandled Exception: Pretzel.Logic.Exceptions.PageProcessingException: Failed to process E:\work\blogging\blog\_site\201
	2\02\07\example-content\index.html, see inner exception for more details ---&amp;gt; DotLiquid.Exceptions.SyntaxException: Unknown tag &apos;gist&apos;
		at DotLiquid.Block.UnknownTag(String tag, String markup, List`1 tokens)
		at DotLiquid.Block.Parse(List`1 tokens)
		at DotLiquid.Document.Initialize(String tagName, String markup, List`1 tokens)
		at DotLiquid.Template.ParseInternal(String source)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why? The reason here is that a sample post in the _posts folder contains, among other bits, the following content&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;{&amp;#37; gist 5555251 gist.md &amp;#37;}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And this does not work in Pretzel.&lt;/p&gt;
&lt;p&gt;Why again? Because Hyde is a Jekyll theme, and Jekyll has a parser for the &lt;strong&gt;gist&lt;/strong&gt; command. Pretzel does NOT. Delete this line from the post.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(However, this also means, you have to figure out an alternative way to embed gists in your post, if you need to)&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;strong&gt;bake&lt;/strong&gt; again with no parameters. Pretzel uses the current directory as the source directory and creates the web-site. Now, &lt;strong&gt;bake&lt;/strong&gt; should succeed, and you should see a sub-folder called &lt;strong&gt;_site&lt;/strong&gt;. This contains your entire blog site.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete the &lt;strong&gt;CNAME&lt;/strong&gt; file from the repository folder as well as the generated &lt;strong&gt;_site&lt;/strong&gt; folder *(This causes issues   if you don&apos;t have a top level domain name. And no, you CANNOT point this to the Github site url).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change &lt;strong&gt;index.html&lt;/strong&gt;. This file should be in the &lt;strong&gt;repo&lt;/strong&gt; directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There should be a line towards the top. md &lt;code&gt;{% for post in paginator.posts %}&lt;/code&gt;. Change &apos;paginator&apos; to &apos;site&apos;. Paginator is a Jekyll plugin and does NOT work in Pretzel. No posts show up if we retain paginator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There should be a line a little below the above &lt;code&gt;&amp;lt;a href=&amp;quot; {{ post.url }}&amp;quot;&amp;gt;&lt;/code&gt;. Change this to &lt;code&gt;&amp;lt;a href=&amp;quot;{{ post.url | prepend: site.baseurl }}&amp;quot;&amp;gt;&lt;/code&gt;. Navigation between posts does not work otherwise.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the config.yml. A config.yml is given below. I changed the &lt;strong&gt;&apos;connect&apos;&lt;/strong&gt; and its sub-items. These fields are used in the Liquid based html templates, viz. &lt;code&gt;&amp;lt;span&amp;gt;{{ site.title }}&amp;lt;/span&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;span&amp;gt;{{ site.connect.github }}&amp;lt;/span&amp;gt;&lt;/code&gt;, etc. The sections marked as &lt;em&gt;(Setup)&lt;/em&gt; in ths file have to be filed in and be accurate. They are used to generate the site.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My config.yml is shown.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yml&quot;&gt;# Dependencies
markdown:         redcarpet
highlighter:      pygments

# Permalinks
permalink:        pretty
relative_permalinks: true

# Setup
title:            &apos;Some text&apos;
tagline:          &apos;Some text&apos;  
description:      &apos;Some text&apos;
url:              &apos;your blog url after publication - use https if https expected, else use http&apos;
baseurl:          &apos;your blog url after publication - use https if https expected, else use http. Could be same or different from previous&apos;

author:
  name:           &apos;a name&apos;
  url:            &apos;a url&apos;

paginate:         5

# Custom vars
version:          1.0.0

connect:
  github:          &apos;@github&apos;
  linkedin:        &apos;@linkedin&apos;
  email:           &apos;@email&apos;

exclude:
  - docs\
  - .gitignore
  - .git
  - pbake.bat
  - ptaste.bat 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;strong&gt;taste&lt;/strong&gt; command as specified above from the repository directory. This command should run, open a browser on local machine and show you the sample posts (ref - we removed the &apos;gist&apos; tag from one of them).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bake and taste the site again - Check the _site and the docs folder. There should be no recursive folder patterns. &lt;em&gt;(Note - this means that the bake and taste commands you used are correct with source and destination file names and other options. Otherwise your site generation times, and github check-in times, and github repo size will exponentially increase with the number of bakes. The two example commands given in usage section are correct except for file-names and avoid the recursive folders issue. I added these two commands as batch files to my repo - pbake.bat and ptaste.bat. These are my build files. Also, since the pretzel bake output is already redirected to the &apos;docs&apos; folder in the batch files, I don&apos;t have to manually copy anything)&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check-in to Github.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Next is what?&lt;/h3&gt;
&lt;p&gt;There&apos;s quite a bit left to do -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A header&lt;/li&gt;
&lt;li&gt;Landing site should show a list of posts, not posts and post-content.&lt;/li&gt;
&lt;li&gt;Disqus integration.&lt;/li&gt;
&lt;li&gt;Google Analytics integration.&lt;/li&gt;
&lt;li&gt;Tag cloud.&lt;/li&gt;
&lt;li&gt;Maybe CNAME and site search.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But for now, this blog seems to be showing up on the internet. If you followed along this far with no problems, you should also be good to go.&lt;/p&gt;
&lt;p&gt;Go forth, and typo. &lt;em&gt;(pssst !! pun intended)&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Additional resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.freeformatter.com/html-entities.html&quot;&gt;Free Formatter&lt;/a&gt; for html encoding.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://daringfireball.net/projects/markdown/syntax&quot;&gt;Markdown Syntax&lt;/a&gt; for reference.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://gist.githubusercontent.com/VEnis/7465176/raw&quot;&gt;Markdown Sample&lt;/a&gt; which works in Github.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Blog like you are coming home.</title>
      <link>https://insen.github.io/blog/2017/08/29/Hello-Blog/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0530</pubDate>
      <author> ([name, insen][url, https://linkedin.com/nilsengupta])</author>
      <guid isPermaLink="true">https://insen.github.io/blog/2017/08/29/Hello-Blog/</guid>
      <description>&lt;h2&gt;Hello blog, eh, I mean, world!&lt;/h2&gt;
&lt;p&gt;As far back as I can remember, I have always been writing. However, I have never blogged. Probably because I preferred the diary and the pen. Having caught a lot of flak for it from my friends, well-wishers, colleagues and bosses alike over the years, I finally decided to fix that.&lt;/p&gt;
&lt;p&gt;But, I had conditions -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not much overhead beyond the writing.&lt;/li&gt;
&lt;li&gt;Retain maximum possible control over content, display and offline support.&lt;/li&gt;
&lt;li&gt;Also, I have a technical background so I wanted to learn something out of it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Note :&lt;/em&gt; The first and the third above may be self-contradictory, but I am ok with that.&lt;/p&gt;
&lt;p&gt;So I started looking. Now, traditional blogging platforms end up keeping my data. I didnt like that.
Plus I havent seen good web-based blog editors. So the web based blogs weren&apos;t options I was inclined to
try first. Having encountered github blogging in the past and liked what I saw, I investigated it again.
Soon, it was clear that the simplicity of a static site was hard to beat. The catch was, Github integrates
Jekyl. Now Jekyll is good, but getting Jekyll on windows involves some fiddling and a good deal of ruby.
Having recently gone through .Net Core, Scala, Spark, Azure and NodeJs recently in quick succession, I had
no appetite for another. A little more looking, and I foundÂ &lt;a href=&quot;https://Github.com/Code52/pretzel&quot;&gt;Pretzel&lt;/a&gt;Â - a .NET
based static-site generator, plus extras.&lt;/p&gt;
&lt;p&gt;Thus I settled.Â &lt;em&gt;(The living-happily-there-ever-after-question has no answers yet.)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blogging should be simple. Hence Github pages.&lt;/li&gt;
&lt;li&gt;Blogging i.e. content, display and offline mode should remain in blogger&apos;s control. Hence pretzel and
not jekyll. Also seeÂ &lt;a href=&quot;https://thomasfreudenberg.com/archive/2016/05/16/from-jekyll-to-pretzel/&quot;&gt;here&lt;/a&gt;Â for
more details.&lt;/li&gt;
&lt;li&gt;And I get to dig into pretzel and maybe learn about static-site-generators.&lt;/li&gt;
&lt;li&gt;Oh, and I rather liked the &lt;a href=&quot;https://github.com/poole/hyde&quot;&gt;Jekyll Hyde Theme&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, this blog is being served from a Github-pages static site, with the site template taken from
the Jekyll theme called Hyde, actually being generated by Pretzel on local machine and getting checked
into Github.&lt;/p&gt;
&lt;p&gt;I just need to figure out markdown now, and then technically speaking, I am back to &lt;strong&gt;write, build,
test and deploy&lt;/strong&gt;. Blog or not, thats like coming home.&lt;/p&gt;</description>
    </item>
    
  </channel> 
</rss>